{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135ae95f",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stavco9/moe-llm-presentation/blob/main/presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28700274-faf1-4b4a-ba3c-159a77d81563",
   "metadata": {
    "id": "28700274-faf1-4b4a-ba3c-159a77d81563"
   },
   "source": [
    "# Mixture of experts Language Model presentation\n",
    "\n",
    "* Course: Seminar in Large Language Models and Information Theory (3968), Master's degree, Computer Science\n",
    "* Institution: Reichman University \n",
    "* Presenters: Noam Delbari & Stav Cohen\n",
    "* Supervise: Dr. Alon Kipnis\n",
    "* High level description: This presentation describes the usage of Mixture of Experts (MoE) in LLM's and includes:\n",
    "  * Background about Transformers - The main model architecture behind LLM's\n",
    "  * Background about Mixture of Experts\n",
    "  * The implementation of MoE using Switch Transformer layer in LLM's\n",
    "  * The implementation of MoE using PEER (parameter efficient expert retrieval) layer in LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6",
   "metadata": {
    "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6"
   },
   "source": [
    "## 1 Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0608-413e-48b7-a512-3b3cc884ca26",
   "metadata": {},
   "source": [
    "### 1.1 Transformers\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/transformer.png\" width=\"300\"/>\n",
    "  <figcaption><b>Figure 1</b> – Transformer architecture.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1756a6a",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "The **Scaled Dot-Product Attention** module computes attention scores between queries $Q$ and keys $K$, scales them by $\\tfrac{1}{\\sqrt{d_k}}$ to stabilize gradients, applies an optional mask (e.g. to ignore padding or future tokens), and then uses softmax-normalized scores to weight the values $V$. This core operation lets each position in the sequence gather information from all other positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe4dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention:\n",
    "      Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch, n_heads, seq_len, d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b493c5",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "The **Multi-Head Attention** module runs several scaled-dot products in parallel (“heads”), allowing the model to jointly attend to information from different representation subspaces. It projects the input into multiple query/key/value spaces, applies attention in each head, concatenates the results, then applies a final linear projection plus residual & layer-norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1904e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head wrapper around ScaledDotProductAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Linear projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, D = x.size()\n",
    "        # project & reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = self.W_Q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        context, attn_weights = self.attn(Q, K, V, mask=mask)\n",
    "\n",
    "        # concat & project back\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.dropout(self.fc_out(context))\n",
    "        out = self.layer_norm(out + x)\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e75c11",
   "metadata": {},
   "source": [
    "#### Position-wise Feed-Forward Network\n",
    "\n",
    "The **Position-wise Feed-Forward** block applies a two-layer MLP to each position independently:\n",
    "$$\n",
    "\\text{FFN}(x) = \\mathrm{Dropout}\\bigl(W_2\\,\\mathrm{ReLU}(W_1 x + b_1) + b_2\\bigr)\n",
    "$$\n",
    "A residual connection and layer normalization follow to stabilize training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer feed-forward network applied per position.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return self.layer_norm(out + x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4168c4",
   "metadata": {},
   "source": [
    "#### Transformer Encoder Layer\n",
    "\n",
    "A single encoder layer consists of:\n",
    "1. **Self-Attention** (Multi-Head Attention over the input sequence)  \n",
    "2. **Feed-Forward** (Position-wise MLP)  \n",
    "Each sublayer uses its own residual connection and layer normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96404a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn       = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        x, _ = self.self_attn(x, mask=src_mask)\n",
    "        x     = self.ffn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbceb1e0",
   "metadata": {},
   "source": [
    "#### Transformer Decoder Layer\n",
    "\n",
    "Each decoder layer has three sublayers:\n",
    "1. **Masked Self-Attention** (prevents future-token attention)  \n",
    "2. **Cross-Attention** (attends to encoder output)  \n",
    "3. **Feed-Forward**  \n",
    "Residual connections and layer norms are applied around each sublayer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn  = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn        = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
    "        x, _ = self.self_attn(x, mask=tgt_mask)\n",
    "        x, _ = self.cross_attn(x=x, mask=memory_mask, x_kv=enc_out)\n",
    "        x    = self.ffn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800c405",
   "metadata": {},
   "source": [
    "#### Full Transformer Model\n",
    "\n",
    "The top-level `Transformer` class orchestrates:\n",
    "1. **Embedding + Positional Encoding** of source and target tokens  \n",
    "2. **Encoder Stack** of $N$ encoder layers  \n",
    "3. **Decoder Stack** of $M$ decoder layers  \n",
    "4. **Final Linear** projection to vocabulary logits for next-token prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ce856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, tgt_vocab_size,\n",
    "                 d_model=512, n_heads=8, d_ff=2048, \n",
    "                 num_enc_layers=6, num_dec_layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # embeddings + positional encoding\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        # encoder & decoder stacks\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_enc_layers)\n",
    "        ])\n",
    "        self.dec_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_dec_layers)\n",
    "        ])\n",
    "\n",
    "        # output projection\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def encode(self, src, src_mask=None):\n",
    "        x = self.positional_encoding(self.src_tok_emb(src))\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, src_mask=src_mask)\n",
    "        return x\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        x = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_out=memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        return x\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encode(src, src_mask)\n",
    "        out    = self.decode(tgt, memory, tgt_mask, memory_mask)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50102d13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04",
   "metadata": {
    "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04"
   },
   "source": [
    "### 1.2 Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a",
   "metadata": {
    "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a"
   },
   "source": [
    "Mixure-of-Experts (MoE) is a machine learning technique where multiple expert networks (learners) are used to divide a problem space into dedicated regions. Rather than a big one network that makes all the required tasks, it's splitted to many dedicated experts which each one of them makes a dedicated task.\n",
    "\n",
    "#### A Brief History of MoEs\n",
    "According to the following paper: https://huggingface.co/blog/moe <br>\n",
    "* The roots of MoEs come from the 1991 paper Adaptive Mixture of Local Experts.\n",
    "* The main idea was to have a supervised procedure for a system composed of separate networks, each handling a different subset of the training cases.\n",
    "* Each separate network, or expert, specializes in a different region of the input space.\n",
    "\n",
    "Between 2010-2015, two different research areas contributed to later MoE advancement (The years where Deep Neural Networks started being used):\n",
    "* Experts as components: In the traditional MoE setup, the whole system includes a gating network and multiple experts\n",
    "* MoEs as the whole model have been explored in ***SVMs***, ***Gaussian Processes***, and other methods\n",
    "* The work by Eigen, Ranzato, and Ilya from Google and NYU explored MoEs as components of deeper networks: This allows having MoEs as layers in a multilayer network, making it possible for the model to be both large and efficient simultaneously.\n",
    "* Conditional Computation: Traditional networks process all input data through every layer. In this period, Yoshua Bengio, a well known deep learning researcher from McGill university in Montréal researched approaches to dynamically activate or deactivate components based on the input token.\n",
    "* These works led to exploring a mixture of experts in the context of NLP.\n",
    "* Concretely, Shazeer et al. (2017, with “et al.” including Geoffrey Hinton and Jeff Dean, Google’s Chuck Norris) scaled this idea to a 137B LSTM (the de-facto NLP architecture back then, created by Schmidhuber) by introducing sparsity, allowing to keep very fast inference even at high scale.\n",
    "* This work focused on translation but faced many challenges, such as high communication costs and training instabilities.\n",
    "\n",
    "#### Some terms\n",
    "1. ***Expert***\n",
    "A small and specialized model which got trained for a particular area. It can be a neural network, decision tree, or other algorithm. In our case experts are small neural networks.\n",
    "\n",
    "3. ***A Mixture of Experts (MoE) model***\n",
    "A model that combines the predictions of multiple experts to solve complex problems.\n",
    "- Each expert is trained on a specific domain or task, and a \"gating network\" or \"router\" selects the most appropriate experts for a given input.\n",
    "\n",
    "3. ***\"gating network\" / \"router\"***\n",
    "A component (a tiny linear layer) in the large model that determines which experts should be activated for a particular input. It's also trained along with the experts\n",
    "\n",
    "#### What do we achieve from that ?\n",
    "The main benefit of the MoE architecture is that it enables large-scale models, even those comprising many billions of parameters, to reduce computation costs during pre-training and achieve faster performance during evaluation time.\n",
    "\n",
    "#### How does it work ?\n",
    "It reaches it's major benefit by selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task.\n",
    "\n",
    "#### An illustration of a standard MoE network\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/moe.png?raw=1\" alt=\"Peer_Layer\" width=\"400\" height=\"400\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/01_moe_layer.png?raw=1\" alt=\"Peer_Layer\" width=\"500\" height=\"600\">\n",
    "</figure>\n",
    "In the left image, the red experts are those who are active\n",
    "\n",
    "---\n",
    "\n",
    "####  Mixture-of-Experts implementation in popular LLMs:\n",
    "\n",
    "##### Mixtral 8×7B (Mistral)\n",
    "**Overview** – Mixtral augments a 7-billion-parameter base transformer with sparse MoE feed-forward blocks to reach **46.7 B total parameters** while keeping the *per-token* cost of roughly a 13 B dense model. It achieves this by letting each token consult only two of eight experts in every layer.\n",
    "\n",
    "* **MoE layout** – Every transformer block swaps its dense FFN for **8 identical SwiGLU experts**  \n",
    "  (hidden size = 14 336).  \n",
    "* **Router** – A learned linear gate $W_g \\in \\mathbb{R}^{d_{\\text{model}}\\times 8}$.\n",
    "\n",
    "  1. For each token $x$: logits $l = x W_g$\n",
    "  2. Keep the **top-2** logits, mask the rest to $-\\infty$\n",
    "  3. Weights $w = \\text{softmax}(l_{\\text{top-2}})$\n",
    "\n",
    "* **Capacity constraint**\n",
    "\n",
    "  $$\n",
    "    \\text{capacity} \\;=\\; \\Bigl\\lceil \\alpha \\,\\frac{T \\cdot K}{N}\\Bigr\\rceil,\n",
    "    \\qquad \\alpha \\approx 1.25\n",
    "  $$\n",
    "\n",
    "  where $T$=tokens in the batch, $K=2$ (top-k), $N=8$ experts.  \n",
    "  Overflow tokens are “zero-routed”.\n",
    "* **Auxiliary Switch loss** equalises both (i) probability mass and (ii) actual token counts per expert.\n",
    "* **Compute cost** – Only $2/8 = 25\\%$ of FFN compute runs per token, so inference costs ≈ 12.9 B-parameter dense model while training exploits the full **46.7 B** capacity.\n",
    "\n",
    "\n",
    "##### DeepSeek-MoE 16 B\n",
    "\n",
    "**Overview** – DeepSeek 16 B pushes specialisation further by *slicing* each FFN into many narrow **sub-experts**: 64 are sparsely routed and 2 are always on. This fine-grained design lets the gate compose highly specific mixtures without raising compute beyond that of a standard 16 B dense transformer.\n",
    "\n",
    "* **Layer composition** – Each MoE layer holds **64 routed sub-experts** + **2 always-on shared sub-experts**.\n",
    "* **Sub-expert** – Same two-layer SwiGLU as a full FFN expert but at **¼ hidden width**, so each is **4× cheaper**.\n",
    "* **Router (routed experts only)**\n",
    "\n",
    "  1. Score the 64 routed sub-experts with a linear gate.  \n",
    "  2. Keep the **top-6**, apply softmax → weights.  \n",
    "  3. Aggregate their weighted outputs.\n",
    "\n",
    "* **Final token output** = weighted sum of **6 routed sub-experts** **+** deterministic outputs of **2 shared sub-experts** ⇒ every token ultimately sees **8 sub-experts**.\n",
    "* **Capacity & balance** – Same ceiling formula as Mixtral (shared experts are exempt); auxiliary loss encourages even traffic across the 64 routed sub-experts.\n",
    "* **Why sub-experts?** Splitting the FFN into many narrow experts gives the router a richer palette of highly specialised functions while staying within the original FLOPs budget.  \n",
    "  Ablations show that removing just a few high-traffic sub-experts hurts perplexity far more than in classic MoE setups, signalling **stronger expert specialisation** and lower redundancy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e70a8c-89fa-4649-83ba-0e205befe346",
   "metadata": {
    "id": "b9e70a8c-89fa-4649-83ba-0e205befe346"
   },
   "source": [
    "## 2 Switch transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d7d88",
   "metadata": {},
   "source": [
    "### 2.1 Introduction:\n",
    "\n",
    "Large language models (LLMs) have achieved striking gains by growing from millions\n",
    "to billions of parameters—yet *dense* scaling makes **every** parameter participate\n",
    "in **every** forward-pass.  \n",
    "Compute (FLOPs), memory traffic, and wall-time therefore grow linearly with model\n",
    "size, and the trillion-parameter frontier strains even the largest clusters.\n",
    "\n",
    "**Mixture-of-Experts (MoE)** layers offer a different path: *conditional\n",
    "computation*.  \n",
    "A lightweight *gating network* selects **one** or **few** specialized “experts”\n",
    "(MLPs) per token, so only a *subset* of parameters is active each step.\n",
    "Switch Transformers (Fedus *et&nbsp;al.*, 2022) refine this idea to make it practical\n",
    "at unprecedented scale.\n",
    "\n",
    "#### Why previous MoE attempts struggled  \n",
    "\n",
    "| Bottleneck in earlier MoE work | What it means | Switch Transformer’s remedy |\n",
    "| :-- | :-- | :-- |\n",
    "| **Unstable top-k routing** (k > 1) | When every token is split across *k* experts (k = 2,4…), the soft mixture may starve some experts of gradient signal → divergence in very deep models. | **k = 1 “switch” routing**: each token is sent to exactly one expert chosen by `argmax` over gate logits. This keeps gradients intact and halves the routing tensor size. |\n",
    "| **Cross-device communication** | Prior systems sliced *one* expert across many GPUs/TPUs → every step required an All-to-All of hidden states. | **Expert-parallel layout**: each device *owns* one whole expert. Tokens are grouped by destination expert, transferred *once*, processed locally, then regrouped—minimising traffic. |\n",
    "| **Token imbalance (hot-spot experts)** | Popular tokens (e.g., punctuation) can overload a few experts, leaving others idle and blowing up memory. | **Auxiliary load-balancing loss**: penalises correlation between (i) fraction of tokens routed to expert *i* and (ii) gate probability mass on expert *i*.  <br>$$ L_{\\text{aux}} = \\alpha\\,N \\sum_{i=1}^{N} f_i\\,P_i \\quad\\text{(Eq.\\;4)} $$ |\n",
    "\n",
    "*Definitions*  \n",
    "* **Expert** – an independent feed-forward sub-network (here, a position-wise MLP).  \n",
    "* **Gating network** – a tiny linear layer that produces *N* logits per token.  \n",
    "* **Routing** – assigning tokens to experts based on gate probabilities.  \n",
    "* **FLOPs / token** – floating-point operations needed for one token’s forward-pass.\n",
    "\n",
    "---\n",
    "\n",
    "#### Research Questions\n",
    "\n",
    "This paper explicitly addresses several critical research questions:\n",
    "\n",
    "1. **Efficiency vs. Model Capacity:**  \n",
    "   *Does increasing model size through sparse routing (more experts) consistently improve language model performance (perplexity) without proportional computational cost?*\n",
    "\n",
    "2. **Training Stability:**  \n",
    "   *Can models at trillion-parameter scale be stably trained using lower-precision arithmetic (like bfloat16)?*\n",
    "\n",
    "3. **Downstream Generalization:**  \n",
    "   *Do improvements obtained during language modeling pre-training generalize to downstream tasks (e.g., QA, summarization)?*\n",
    "\n",
    "4. **Model Compression and Deployment:**  \n",
    "   *Is it feasible to distill sparse models into smaller, dense models while preserving performance improvements?*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff624e5",
   "metadata": {},
   "source": [
    "### 2.2 Model architecture overview  \n",
    "\n",
    "A Switch Transformer layer is identical to a standard Transformer block **except** that the\n",
    "dense Feed-Forward Network (FFN) is replaced by a **Switch-FFN** (sparse Mixture-of-Experts).  \n",
    "Figure 2 from the paper illustrates the encoder block with two example tokens (${x_1,x_2}$).  \n",
    "Only the shaded *Switch-FFN* (light-blue) differs from a dense model.  :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure2_switch_arch.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure 2</b> – Switch-FFN inside the Transformer block (Fedus et al., 2022).</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15977d0",
   "metadata": {},
   "source": [
    "#### 2.2.1 Switch Transformer Module\n",
    "\n",
    "The `SwitchTransformer` builds on the classic Transformer encoder by:\n",
    "\n",
    "1. **Embedding + Positional Encoding**  \n",
    "   - Converts token IDs into \\(d_{\\text{model}}\\)-dimensional vectors and adds sinusoidal (or learned) timing signals.\n",
    "\n",
    "2. **Layer Stack**  \n",
    "   - Clones a prototype `SwitchTransformerLayer` \\(L\\) times, enabling sparse, per-token expert routing at each depth.  \n",
    "   - Gathers routing statistics (`counts`, `load`, `dropped`, `max_p`) for every layer to support the auxiliary load-balancing loss.\n",
    "\n",
    "3. **Final Normalization**  \n",
    "   - Applies a top-level `LayerNorm` to stabilize outputs across the full depth.\n",
    "\n",
    "This architecture maintains the Transformer’s depth and self-attention benefits while drastically increasing capacity via \\(N\\) experts, all at constant computational cost per token.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b50fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SwitchTransformer(Module):\n",
    "    \"\"\"\n",
    "    Stacks multiple SwitchTransformerLayer’s into a full encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 src_vocab_size: int, d_model: int, n_heads: int,\n",
    "                 d_ff: int, n_layers: int,\n",
    "                 capacity_factor: float, drop_tokens: bool,\n",
    "                 is_scale_prob: bool, n_experts: int,\n",
    "                 max_seq_len: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        # Token embedding + positional encoding\n",
    "        self.token_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos_enc   = PositionalEncoding(d_model, dropout_prob, max_seq_len)\n",
    "\n",
    "        # Stack of SwitchTransformerLayers\n",
    "        layer = SwitchTransformerLayer(\n",
    "            d_model=d_model, n_heads=n_heads, d_ff=d_ff,\n",
    "            capacity_factor=capacity_factor, drop_tokens=drop_tokens,\n",
    "            is_scale_prob=is_scale_prob, n_experts=n_experts,\n",
    "            dropout_prob=dropout_prob\n",
    "        )\n",
    "        self.layers    = clone_module_list(layer, n_layers)\n",
    "        self.norm      = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None):\n",
    "        # 1) Embed & add positional encoding\n",
    "        x = self.token_emb(src) * math.sqrt(self.token_emb.embedding_dim)\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        # 2) Apply each SwitchTransformerLayer\n",
    "        all_stats = []\n",
    "        for layer in self.layers:\n",
    "            x, counts, load, dropped, max_p = layer(x, mask=src_mask)\n",
    "            all_stats.append({\n",
    "                'counts': counts,\n",
    "                'load': load,\n",
    "                'dropped': dropped,\n",
    "                'max_p': max_p\n",
    "            })\n",
    "\n",
    "        # 3) Final layer normalization\n",
    "        x = self.norm(x)\n",
    "        return x, all_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7069f0",
   "metadata": {},
   "source": [
    "#### 2.2.2 Switch TransformerLayer Module\n",
    "\n",
    "This layer mirrors a standard Transformer block but replaces the fixed-position FFN with a sparse “SwitchFeedForward” mixture-of-experts:\n",
    "\n",
    "1. **Self-Attention Sub-Layer**  \n",
    "   - Applies pre-norm, multi-head self-attention (`MultiHeadAttention`) to allow tokens to mix contextual information.  \n",
    "   - Adds a residual connection and dropout.\n",
    "\n",
    "2. **Mixture-of-Experts Feed-Forward Sub-Layer**  \n",
    "   - Applies pre-norm, then routes each token through exactly one of $N$ expert FFNs via `SwitchFeedForward`.  \n",
    "   - Collects routing statistics (`counts`, `load`, `dropped`, `max_p`) for auxiliary losses.  \n",
    "   - Adds a residual connection and dropout.\n",
    "\n",
    "Each sublayer uses its own `LayerNorm` and the same `dropout_prob` for stability and regularization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SwitchTransformerLayer(Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block using SwitchFeedForward instead of a fixed FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, d_model: int, n_heads: int, d_ff: int,\n",
    "                 capacity_factor: float, drop_tokens: bool,\n",
    "                 is_scale_prob: bool, n_experts: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        # 1) Multi-head self-attention sublayer\n",
    "        self.attn       = MultiHeadAttention(d_model, n_heads, dropout_prob)\n",
    "        # 2) Mixture-of-experts feed-forward sublayer\n",
    "        base_ffn        = PositionwiseFeedForward(d_model, d_ff, dropout_prob)\n",
    "        self.feed_forward = SwitchFeedForward(\n",
    "            capacity_factor=capacity_factor,\n",
    "            drop_tokens=drop_tokens,\n",
    "            is_scale_prob=is_scale_prob,\n",
    "            n_experts=n_experts,\n",
    "            expert=base_ffn,\n",
    "            d_model=d_model\n",
    "        )\n",
    "        # Layer norms & dropout\n",
    "        self.norm1      = nn.LayerNorm(d_model)\n",
    "        self.norm2      = nn.LayerNorm(d_model)\n",
    "        self.dropout    = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None):\n",
    "        # 1) Pre-norm + self-attention\n",
    "        z1, _      = self.attn(self.norm1(x), mask=mask)\n",
    "        x          = x + self.dropout(z1)\n",
    "\n",
    "        # 2) Pre-norm + switch-FFN\n",
    "        z2, counts, load, dropped, max_p = self.feed_forward(self.norm2(x))\n",
    "        x          = x + self.dropout(z2)\n",
    "\n",
    "        return x, counts, load, dropped, max_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e57cd",
   "metadata": {},
   "source": [
    "#### 2.2.3 Switch FeedForward Module\n",
    "Initializes the mixture-of-experts block by:\n",
    "- Replicating the base feed-forward network into `n_experts` separate experts.\n",
    "- Defining a lightweight router (`self.switch`) that maps each token embedding of size `d_model` to unnormalized logits for each expert.\n",
    "- Preparing to convert those logits into a probability distribution with `Softmax`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class SwitchFeedForward(Module):\n",
    "    def __init__(self, *, capacity_factor: float, drop_tokens: bool,\n",
    "                 is_scale_prob: bool, n_experts: int,\n",
    "                 expert: FeedForward, d_model: int):\n",
    "        super().__init__()\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.drop_tokens     = drop_tokens\n",
    "        self.is_scale_prob   = is_scale_prob\n",
    "        self.n_experts       = n_experts\n",
    "        # 1) Create N independent copies of the base FFN\n",
    "        self.experts = clone_module_list(expert, n_experts)\n",
    "        # 2) Router: a linear map to produce logits for each expert\n",
    "        self.switch  = nn.Linear(d_model, n_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4699f1",
   "metadata": {},
   "source": [
    "##### Forward funciton\n",
    "\n",
    "This function implements the forward pass of the `SwitchFeedForward` mixture-of-experts layer. It accepts an input tensor `x` of shape `[seq_len, batch, d_model]`, where:\n",
    "\n",
    "- `seq_len` is the sequence length  \n",
    "- `batch` is the batch size  \n",
    "- `d_model` is the model’s hidden dimension  \n",
    "\n",
    "Inside, each token is separated out for per-token routing across experts; the method ultimately returns both the transformed output and auxiliary routing statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6219c56",
   "metadata": {},
   "source": [
    "##### Flatten Inputs\n",
    "\n",
    "Reshapes input from shape `[seq_len, batch, d_model]` into `[T, d_model]` with `T = seq_len × batch`, so that each token is routed independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "    def forward(self, x: Tensor):\n",
    "        seq_len, batch, d_model = x.shape\n",
    "        # Collapse sequence and batch dims for per-token routing\n",
    "        flat_x = x.view(-1, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3d315",
   "metadata": {},
   "source": [
    "##### Compute Raw Routing Logits\n",
    "\n",
    "Each token embedding `x` is linearly projected to produce unnormalized scores:\n",
    "$$\n",
    "h_i(x) = \\bigl[W_{\\text{switch}}\\,x + b\\bigr]_i,\\quad i=1,\\dots,N.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317314b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        logits = self.switch(flat_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffe4ac",
   "metadata": {},
   "source": [
    "##### Normalize into Routing Probabilities\n",
    "\n",
    "Applies softmax to the logits to yield a distribution over experts:\n",
    "$$\n",
    "p_i(x) = \\frac{\\exp\\bigl(h_i(x)\\bigr)}{\\sum_{j=1}^N \\exp\\bigl(h_j(x)\\bigr)},\\quad \\sum_i p_i(x)=1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200a860",
   "metadata": {},
   "source": [
    " %%\n",
    "        route_prob = self.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05563ed1",
   "metadata": {},
   "source": [
    "##### Pick Top-1 Expert\n",
    "\n",
    "Routes each token to the expert with maximum probability:\n",
    "$$\n",
    "i^*(x) = \\arg\\max_i\\,p_i(x),\\qquad p_{i^*}(x)=\\max_i p_i(x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        prob_max, routes = torch.max(route_prob, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f5376",
   "metadata": {},
   "source": [
    "##### Compute Expert Capacity\n",
    "\n",
    "Determines the per-expert capacity:\n",
    "$$\n",
    "C = \\Bigl\\lfloor \\alpha\\times\\frac{T}{N}\\Bigr\\rfloor,\n",
    "$$\n",
    "where $T$ is total tokens, $N$ experts, and $\\alpha$ is `capacity_factor`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        capacity = int(self.capacity_factor * flat_x.size(0) / self.n_experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc249f6b",
   "metadata": {},
   "source": [
    "##### Enforce Capacity & Optionally Drop\n",
    "\n",
    "1. **Counts:** number of tokens routed to each expert.  \n",
    "2. If `drop_tokens=True`, any expert receiving more than $C$ tokens will randomly drop the excess; dropped tokens bypass later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59234007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        counts  = torch.tensor([len((routes==i).nonzero()) for i in range(self.n_experts)])\n",
    "        dropped = []\n",
    "        if self.drop_tokens:\n",
    "            for i in range(self.n_experts):\n",
    "                idxs = (routes==i).nonzero(as_tuple=True)[0]\n",
    "                if idxs.numel() > capacity:\n",
    "                    perm = idxs[torch.randperm(idxs.numel())]\n",
    "                    dropped.append(perm[capacity:])\n",
    "                    idxs = perm[:capacity]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdee90",
   "metadata": {},
   "source": [
    "##### Dispatch Tokens to Experts\n",
    "Applies each expert $E_i$ only to its assigned subset $\\mathcal{I}_i$:\n",
    "$$\n",
    "y_i = E_i\\bigl(x_{\\mathcal{I}_i}\\bigr).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81287ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        final_out = flat_x.new_zeros(flat_x.shape)\n",
    "        for i in range(self.n_experts):\n",
    "            idxs = (routes==i).nonzero(as_tuple=True)[0]\n",
    "            if idxs.numel() > 0:\n",
    "                out_i = self.experts[i](flat_x[idxs])\n",
    "                final_out[idxs] = out_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7fbe2",
   "metadata": {},
   "source": [
    "##### Handle Dropped Tokens\n",
    "\n",
    "Tokens dropped due to capacity limits are passed through unchanged:\n",
    "$$\n",
    "y_{\\mathrm{bypass}}(x) = x.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855dfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        if dropped:\n",
    "            all_dropped = torch.cat(dropped)\n",
    "            final_out[all_dropped] = flat_x[all_dropped]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856285dc",
   "metadata": {},
   "source": [
    "#####  Scale by Gate Value\n",
    "\n",
    "Modulates each token’s expert output by its gate value:\n",
    "$$\n",
    "\\tilde y(x) = p_{i^*}(x)\\,y_{i^*}(x).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ced04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        if self.is_scale_prob:\n",
    "            final_out = final_out * prob_max.unsqueeze(-1)\n",
    "        else:\n",
    "            final_out = final_out * (prob_max/prob_max.detach()).unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81778e",
   "metadata": {},
   "source": [
    "#####  Restore Shape & Return\n",
    "\n",
    "Reshapes the result back to `[seq_len, batch, d_model]` and returns auxiliary statistics for load-balancing loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff54e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "        output = final_out.view(seq_len, batch, d_model)\n",
    "        return output, counts, route_prob.sum(0), sum(len(d) for d in dropped), prob_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691aa1d",
   "metadata": {},
   "source": [
    "####  2.2.4 **Capacity and Hard Limit**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure3_capacity_dynamics.png\" width=\"650\"/>\n",
    "  <figcaption><b>Figure 3 — Token-routing dynamics under two capacity factors.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "**What the diagram shows**\n",
    "\n",
    "*Left panel (capacity 1.0)*  \n",
    "* 12 tokens must be routed across three experts (rows).  \n",
    "* Each expert’s capacity is exactly the ideal load \\(B/N = 4\\) tokens.  \n",
    "* Because many tokens happen to share the same favourite expert, that expert\n",
    "  overflows — the dotted red boxes show the **dropped** tokens that will bypass\n",
    "  this layer.\n",
    "\n",
    "*Right panel (capacity 1.5)*  \n",
    "* The capacity per expert is now \\(4 \\times 1.5 = 6\\) tokens, giving 50 % slack.  \n",
    "* All 12 tokens fit; no red overflow boxes, but a few **empty white slots** indicate\n",
    "  wasted compute/communication.\n",
    "\n",
    "**Take-away** A small slack margin (the paper standardises on 1.25) almost\n",
    "eliminates overflows yet keeps extra FLOPs and bandwidth modest.  Capacity is\n",
    "therefore the runtime *circuit-breaker* that guarantees fixed memory and latency\n",
    "even when the router’s token-to-expert distribution is skewed.\n",
    "\n",
    "**Why does capacity matter?**\n",
    "\n",
    "Routing decisions can lead to unbalanced token assignments, with some experts overloaded while others remain underutilized. Without mitigation, overloaded experts would cause out-of-memory errors, degraded performance, and unpredictable latency.\n",
    "\n",
    "Switch Transformer addresses this via a **hard capacity limit** on each expert:\n",
    "\n",
    "$$\n",
    "\\text{capacity per expert} = \\left\\lceil \\frac{B}{N} \\times \\text{capacity\\_factor} \\right\\rceil\n",
    "$$\n",
    "\n",
    "- $ B $: Total number of tokens in the micro-batch\n",
    "- $ N $: Number of experts\n",
    "- $\\text{capacity\\_factor}$: Typically set to $1.25$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb742a",
   "metadata": {},
   "source": [
    "\n",
    "####  2.2.5 **Load-Balancing Losses**\n",
    "\n",
    "Early attempts at Mixture-of-Experts models encountered a critical problem known as **expert collapse**, where a small number of experts dominated token assignments, starving others of training signal. Switch Transformers introduced two auxiliary losses to prevent collapse and encourage balanced expert usage:\n",
    "\n",
    "##### 1. **Auxiliary Load-Balancing Loss**\n",
    "\n",
    "$$\n",
    "L_{\\text{aux}} = \\alpha N \\sum_{i=1}^{N} f_i P_i,\\quad\n",
    "f_i = \\frac{\\text{tokens routed to expert } i}{\\text{batch size}},\\quad\n",
    "P_i = \\text{average gate probability for expert } i\n",
    "$$\n",
    "\n",
    "Encourages even token distribution by penalizing high correlation between an expert’s frequency of selection and router confidence.\n",
    "\n",
    "You can think of $\\mathbf f^\\top \\mathbf P$ as the **expected confidence** the router has in the experts it actually chooses.  \n",
    "- If the router always picks highly‐confident experts (i.e.\\ $\\mathbf P$ is very “peaked”), then $\\mathbf f^\\top \\mathbf P$ will be high.  \n",
    "- By **penalizing** $\\sum_{i=1}^N f_i\\,P_i = \\mathbf f^\\top \\mathbf P$, we force the router to **spread its confidence** more evenly across all experts.  \n",
    "- As a result, the actual assignments $\\mathbf f$ become more balanced, improving expert utilization and reducing hardware bottlenecks.\n",
    "\n",
    "\n",
    "##### 2. **Z-Loss (Logit Regularization)**\n",
    "\n",
    "\n",
    "$$\n",
    "L_z = \\beta \\sum_{\\text{tokens}}\\sum_{i}(h_i(token_i) - \\text{stop\\_grad}(token_i))^2\n",
    "$$\n",
    "\n",
    "**Intuition:**  \n",
    "- Encourages router logits $h_i$ to remain small in magnitude, preventing overly confident selections (extremely large or small logits can destabilize softmax distributions).\n",
    "- Maintains stable gradient flow and well-conditioned router softmax.\n",
    "\n",
    "\n",
    "1. **Definition of** $\\mathrm{stop\\_grad}(x)$:  \n",
    "   $$\n",
    "     \\mathrm{stop\\_grad}(x)\\;=\\;x\n",
    "     \\quad\\text{(forward value)}\\,,\\qquad\n",
    "     \\frac{\\partial\\,\\mathrm{stop\\_grad}(x)}{\\partial x}=0\n",
    "     \\quad\\text{(blocks gradients)}\n",
    "   $$\n",
    "\n",
    "2. **Corrected Z-Loss** for a target logit $h_i(x)$:  \n",
    "   $$\n",
    "     \\mathcal{L}_Z\n",
    "     \\;=\\;\\beta\\;\\bigl(h_i(x)\\;-\\;\\mathrm{stop\\_grad}\\bigl[\\log\\!\\sum_{j=1}^N e^{h_j(x)}\\bigr]\\bigr)^{2}.\n",
    "   $$\n",
    "\n",
    "3. **Relation to log-softmax**:  \n",
    "   $$\n",
    "     h_i(x)-\\log\\!\\sum_{j=1}^N e^{h_j(x)}\n",
    "     =\\log\\!\\bigl(e^{h_i(x)}\\bigr)\\;-\\;\\log\\!\\sum_{j=1}^N e^{h_j(x)}\n",
    "     =\\log p_i(x),\n",
    "   $$  \n",
    "   so this difference directly measures the **log-probability** of expert $i$ under the softmax.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a96b60",
   "metadata": {},
   "source": [
    "### 2.3 Experiments & Results\n",
    "#### Baseline Models Used for Comparison  \n",
    "\n",
    "The authors selected **four families** of comparison models, each serving a distinct purpose.\n",
    "\n",
    "####   Dense T5 Series – \n",
    "**T5** stands for **“Text-to-Text Transfer Transformer.”**  \n",
    "Released by Google in late 2019, it introduced a simple yet powerful idea: *cast every NLP task—translation, summarisation, QA, sentiment, …—as feeding one piece of text in and predicting another piece of text out.*  \n",
    "This unification plus large-scale span-corruption pre-training on the **C4** web corpus produced a strong encoder–decoder baseline.\n",
    "\n",
    "**Why the authors picked T5:**\n",
    "\n",
    "1. **Like-for-like objective and codebase** – eliminates spurious gains from task formulation or optimiser tweaks.  \n",
    "2. **Widely reported benchmarks** – GLUE / SuperGLUE / SQuAD scores for T5 are standard yard-sticks, so improvements are easy to contextualise.  \n",
    "3. **Scales up smoothly** – letting the paper test whether sparse scaling beats a dense model simply made *bigger* (e.g., T5-Large or T5-XXL).\n",
    "\n",
    "Thus, throughout the experiments T5 provides a **clean, well-understood dense baseline** against which the efficiency and quality of the Switch (sparse) approach can be judged.\n",
    "\n",
    "\n",
    "| Model | Params | FLOPs / token | Why chosen |\n",
    "|-------|--------|--------------|-----------|\n",
    "| **T5-Base** | 223 M | 1 × (reference) | Same size class as 2-expert Switch; establishes a dense baseline that already fits on a single TPU/GPU. |\n",
    "| **T5-Large** | 739 M | 3.5 × Switch-Base | Represents a “scale-up dense” strategy within the same architecture and codebase. |\n",
    "| **T5-XXL** | 11 B | 6.3 T per seq | State-of-the-art dense model at publication time; tests whether sparse can outpace *very* large dense models under the same cluster budget. |\n",
    "\n",
    "*Rationale* – All T5 variants share the *exact* training objective, tokenizer, and optimizer code. That isolates the effect of **conditional vs dense compute**.\n",
    "\n",
    "#####   MoE Transformer (Top-2 Routing) – \n",
    "\n",
    "| Variant | Experts | Routing | Why chosen |\n",
    "|---------|---------|---------|-----------|\n",
    "| **MoE-Transformer (Shazeer et al.)** | 128 | top-2 | Prevailing MoE design before Switch; higher FLOPs because two experts fire per token. |\n",
    "\n",
    "*Rationale* – Validates whether **single-expert** routing is genuinely more efficient/stable than the established top-k approach.\n",
    "\n",
    "##### Why these baselines are fair  \n",
    "\n",
    "* **Same tokenizer and data** → eliminates corpus effects.  \n",
    "* **Same optimizer hyper-params** (where feasible) → isolates architectural difference.  \n",
    "* **FLOP-matched pairs** (Switch-Base vs T5-Base, Switch-Large vs T5-Large) → asks:  \n",
    "  > *“Given the **same compute budget**, which architecture learns faster / better?”*  \n",
    "* **Higher-FLOP dense models** (T5-Large, -XXL) → test the critique  \n",
    "  > *“Just spend more FLOPs on dense; why bother with sparsity?”*  \n",
    "* **Legacy MoE top-2** → ensures the improvement isn’t merely “MoE vs dense” but due to the **Switch simplification**.\n",
    "\n",
    "Using this spectrum of baselines, the paper demonstrates that Switch Transformers outperform:\n",
    "\n",
    "1. **Compute-matched dense** models (fair efficiency test),  \n",
    "2. **Heavier dense** models (efficiency-vs-quality frontier), and  \n",
    "3. **Previous sparse** architectures (methodological advance).\n",
    "\n",
    "This comprehensive baseline suite strengthens the claim that **conditional compute via single-expert routing is a superior scaling path**.\n",
    "\n",
    "####  Scaling Properties \n",
    "The paper’s **Scaling Properties** section asks:  \n",
    "> *“If we keep FLOPs / token roughly constant, how far can we improve quality by adding more experts (i.e., more parameters)?”*  \n",
    "\n",
    "To answer, the authors run three tightly-controlled experiments.\n",
    "\n",
    "#####  Step-Basis Scaling\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_4_step_basis_scaling.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure: Scaling Switch Transformer (perplexity vs. training steps and wall-clock).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Pre-train **Switch-Base** models with 2 → 256 experts (223 M → 14.7 B parameters) **for a fixed 100 k steps**. FLOPs/token stay constant because each token still activates one expert. |\n",
    "| **Purpose** | Is parameter-only scaling (via experts) a free win when compute is fixed? |\n",
    "| **Main results** | Perplexity **drops monotonically** as experts double. The 64-expert model matches T5-Base quality **7.5 × sooner** in steps. |\n",
    "| **Interpretation** | Extra capacity (parameters) is effectively used even though compute is unchanged. Sparse routing is therefore a *new scaling axis* orthogonal to FLOPs. |\n",
    "\n",
    "#####  Time-Basis Scaling\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_5_time_basis_scaling.png\" width=\"300\"/>\n",
    "  <figcaption><b>Figure: Scaling Switch Transformer (perplexity vs. training time and wall-clock).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Measure **wall-clock minutes** to reach target perplexities on identical TPU pods. Same model family as above. |\n",
    "| **Purpose** | Extra experts add routing overhead (softmax, All-to-All). Do they erase the step advantage? |\n",
    "| **Main results** | Sparse models *still* win: 64-expert Switch reaches T5-Base quality in **≈ 140 min vs 350 min** (≈ 2.5× faster). |\n",
    "| **Conclusion** | Routing + communication overhead is small relative to the gains from parameter scaling. Sparse models give **real-time savings**, not just step savings. |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#####  Sparse vs. “Just Make the Dense Model Bigger”\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_6_parameters_basis_scaling.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure: Sample Efficiency Switch Transformer VS T5 variants.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Compare **Switch-Base (64 e)** against **T5-Large** which spends **3.5× more FLOPs/token** than Switch-Base. |\n",
    "| **Purpose** | Critics could argue “dense scaling already works—just spend more FLOPs.” |\n",
    "| **Main results** | Despite T5-Large’s heavier compute, Switch-Base is **2.5× faster** to the same perplexity and *still* ends lower. |\n",
    "| **Conclusion** | Conditional computation **dominates** naive dense scaling in the speed/quality trade-off. You can’t buy the same improvement just by burning more FLOPs per token. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759c905",
   "metadata": {},
   "source": [
    "####  Down-Stream Experiments\n",
    "\n",
    "The authors performed five focused studies to verify that the pre-training gains of Switch Transformers **transfer** to real tasks and to understand how best to fine-tune, regularise, and deploy very large sparse models.\n",
    "\n",
    "\n",
    "#####  Fine-Tuning Benchmark Suite\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/table_5_fine_tuning_results.png\" width=\"400\"/>\n",
    "  <figcaption><b>Table: Fine-tuning results. T5 baselines VS Switch models across\n",
    "  a diverse set of natural language test.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "- **GLUE —** A bundle of nine sentence‐level and sentence-pair evaluations (sentiment, paraphrase, natural-language inference, etc.) that together gauge broad language understanding in English.\n",
    "\n",
    "- **SuperGLUE —** A harder successor to GLUE featuring multi-sentence reasoning tasks such as BoolQ, ReCoRD, and WSC; designed to test deeper compositional reasoning and commonsense.\n",
    "\n",
    "- **SQuAD v1.1 —** Reading-comprehension question answering on Wikipedia passages; checks the model’s ability to locate and extract exact answer spans from context.\n",
    "\n",
    "- **XSum —** Single-sentence abstractive news summarisation; evaluates whether the system can condense an article into one concise, fluent sentence while preserving key facts.\n",
    "\n",
    "- **Winogrande —** Commonsense pronoun-resolution puzzles; measures the model’s grasp of implicit world knowledge needed to resolve ambiguous references.\n",
    "\n",
    "- **TriviaQA (closed-book) —** Open-domain factoid QA answered without external documents; probes how much factual knowledge is stored internally in the model’s parameters.\n",
    "\n",
    "- **ANLI —** Adversarial natural-language inference collected via model-in-the-loop annotation; assesses robustness to deliberately tricky NLI examples.\n",
    "\n",
    "- **ARC (Easy & Challenge) —** Multiple-choice grade-school science-exam questions; tests logical reasoning over short factual statements rather than surface pattern matching.\n",
    "\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Fine-tuned FLOP-matched pairs: **Switch-Base (7 B) vs T5-Base (0.2 B)** and **Switch-Large (26 B) vs T5-Large (0.7 B)** on GLUE, SuperGLUE, SQuAD, XSum, Winogrande, TriviaQA, ANLI, ARC. Dropout: 0.1 non-expert, 0.4 expert; 100 k training steps. |\n",
    "| **Purpose** | Confirm that sparse pre-training advantages appear on diverse NLU, QA, and summarisation tasks. |\n",
    "| **Main results** | • **+4.4 pp SuperGLUE** (Base) and +2 pp (Large).<br>• Closed-book TriviaQA +6 pp.<br>• Gains on Winogrande, XSum; mixed on ARC. |\n",
    "| **Conclusion** | Pre-training gains **transfer broadly**; sparse capacity especially helps knowledge-heavy tasks. |\n",
    "\n",
    "####  Distillation for Deployment\n",
    "\n",
    "##### How knowledge distillation works in general  \n",
    "\n",
    "1. **Run the teacher** – pass each input through the (large) teacher model  \n",
    "   * obtain either the **logits** $z^{(T)}$ or the softened probabilities  \n",
    "     $\\sigma(z^{(T)}/T)$ at temperature $T>1$.\n",
    "\n",
    "2. **Run the student** – pass the *same* input through the small model  \n",
    "   to produce logits $z^{(S)}$.\n",
    "\n",
    "3. **Blend two losses**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "  = \\lambda \\; \\underbrace{\\mathrm{KL}\\!\\bigl[\\sigma(z^{(T)}/T)\\;\\|\\;\\sigma(z^{(S)}/T)\\bigr]}_{\\text{soft-target loss}}\n",
    "  + (1-\\lambda)\\; \\underbrace{\\mathrm{CE}\\!\\bigl[y,\\;\\sigma(z^{(S)})\\bigr]}_{\\text{hard-target loss}}\n",
    "$$\n",
    "\n",
    "* $y$ – ground-truth labels  \n",
    "* $\\sigma$ – softmax  \n",
    "* $\\lambda$ – weight that trades off “mimic the teacher” vs. “fit the labels”  \n",
    "* Back-propagate **only through the student**; the teacher is frozen.\n",
    "\n",
    "4. **Optimise the student** until validation perplexity or task metric plateaus.  \n",
    "   The student thus learns a compressed approximation of the teacher’s behaviour\n",
    "   while still respecting the original task labels.\n",
    "\n",
    "\n",
    "##### Switch-Transformer distillation  \n",
    "\n",
    "* **Teacher models** – sparse Switch-Base variants  \n",
    "  * 3.8 B , 7.4 B , 14.7 B parameters (64 or 128 experts)  \n",
    "  * already pre-trained on C4; one run fine-tuned on SuperGLUE.\n",
    "\n",
    "* **Student model** – dense T5-Base, 223 M parameters.\n",
    "\n",
    "* **Weight initialisation** – copy all **non-expert weights** (embeddings, attention,\n",
    "  residual projections) from teacher to student; randomly init the rest.\n",
    "\n",
    "* **Soft / hard mix** –  \n",
    "  $\\lambda = 0.25$ for the soft-target KL term, $1-\\lambda = 0.75$ for the standard cross-entropy.\n",
    "\n",
    "* **Temperature** – $T = 2.0$ to soften the teacher’s probability distribution.\n",
    "\n",
    "* **Training data & length** –  \n",
    "  * C4 span-corruption for language modelling distillation (150 k steps).  \n",
    "  * SuperGLUE labelled set for task-specific distillation (same step budget).\n",
    "\n",
    "* **Optimiser & schedule** – identical Adafactor settings used in pre-training; no extra tricks.\n",
    "\n",
    "* **Outcome** – student keeps **≈30 %** of the teacher’s quality gain while shrinking\n",
    "  model size by **95–99 %**, demonstrating a deployable path for Switch-Transformer\n",
    "  knowledge.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/table_6_distillition.png\" width=\"400\"/>\n",
    "  <figcaption><b>Table: Fine-tuning results. T5 baselines VS Switch models across\n",
    "  a diverse set of natural language test.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **What was done** | Distilled sparse **Switch-Base 3.8 B / 7.4 B / 14.7 B** teachers into a 223 M dense T5-Base student. Tricks: initialise student with teacher’s non-expert weights + 0.25 × soft-loss + 0.75 × hard-loss. |\n",
    "| **Why** | Massive trillion-parameter models are hard to deploy; distillation offers a lighter alternative. |\n",
    "| **Main results** | • **≈ 30 % of teacher gain retained** at 95–99 % compression.<br>• Fine-tuned SuperGLUE distillation keeps 30 % gain on a 97 % compressed model. |\n",
    "| **Interpretation** | Distillation provides a **practical path** from huge sparse teachers to deployable dense students while preserving a meaningful slice of quality improvement. |\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9eb59",
   "metadata": {},
   "source": [
    "### 2.4 Switch Extensions: MixLoRA & MoE-Mamba\n",
    "\n",
    "#### MixLoRA: LoRA-Based Sparse MoE (Li et al., 2024)\n",
    "\n",
    "**At a glance**  \n",
    "A parameter-efficient sparse MoE that injects LoRA adapters as experts into a frozen backbone for multi-task fine-tuning on modest GPUs.\n",
    "\n",
    "**What is LoRA?**  \n",
    "LoRA (Low-Rank Adaptation) freezes a large weight matrix $W\\in\\mathbb{R}^{d\\times d}$ and learns two much smaller matrices $A\\in\\mathbb{R}^{d\\times r}$ and $B\\in\\mathbb{R}^{r\\times d}$ with $r\\ll d$, such that\n",
    "$$\n",
    "W' = W + B\\,A.\n",
    "$$\n",
    "\n",
    "**Switch-inspired element**  \n",
    "- **Auxiliary load-balancing loss** to encourage uniform expert utilization.\n",
    "\n",
    "**Key improvements**  \n",
    "- **Top-2 gating** for richer expert selection.  \n",
    "- **Independent attention-layer adapters** for per-layer specialization.  \n",
    "- **40 % GPU memory reduction** and **30 % lower latency**.  \n",
    "- **9 % accuracy gain** on multi-task benchmarks.\n",
    "\n",
    "#### MoE-Mamba: SSM-Infused Sparse MoE (Pióro et al., 2024)\n",
    "\n",
    "**At a glance**  \n",
    "A hybrid SSM/MoE model that interleaves Mamba State-Space Model blocks with true Switch-style sparse experts.\n",
    "\n",
    "**What is an SSM?**  \n",
    "A State-Space Model evolves a hidden state $h_t$ via  \n",
    "$$\n",
    "h_t = A\\,h_{t-1} + B\\,u_t,\\quad\n",
    "y_t = C\\,h_t + D\\,u_t,\n",
    "$$  \n",
    "where $u_t$ is the input and $y_t$ the output.\n",
    "\n",
    "**Innovation in Mamba**  \n",
    "- **Input-dependent parameterization** of $A,B,C,D$ for content-based gating.  \n",
    "- **Parallel-scan (FFT-style) solver** achieving $O(N\\log N)$ inference on long sequences.  \n",
    "- **Diagonal + low-rank decomposition** for compact, efficient long-range modeling.\n",
    "\n",
    "**Switch-inspired elements**  \n",
    "- **Single-expert routing** ($k=1$) with capacity-factor buffering.  \n",
    "- **Auxiliary load-balancing loss** at every MoE layer.  \n",
    "- **Low-precision routing** (float32 jitter) for stable training.\n",
    "\n",
    "**Key improvements**  \n",
    "- **Sequential interleaving** of SSM and MoE blocks.  \n",
    "- **Extensive ablations** on expert count, placement, and parameter ratios.  \n",
    "- **2.35× fewer training steps** to match Mamba perplexity and outperforms Transformer-MoE.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c02a4",
   "metadata": {},
   "source": [
    "### 2.5 Contributions\n",
    "\n",
    "The paper's main contributions are:\n",
    "\n",
    "- Introducing **single-expert (top-1) routing**, drastically simplifying routing and reducing computational overhead.\n",
    "- Presenting clear evidence of performance improvements across diverse NLP benchmarks, validating the model’s practical utility.\n",
    "- Proposing effective model compression through knowledge distillation, enabling deployment of large model capabilities into significantly smaller models.\n",
    "\n",
    "Together, these findings validate the viability of conditional computation at unprecedented scale.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167407-c9a5-429f-87fb-1de5c50a6951",
   "metadata": {
    "id": "57167407-c9a5-429f-87fb-1de5c50a6951"
   },
   "source": [
    "## 3 PEER (parameter efficient expert retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9f11-e31a-41a5-b8e6-813bf37d7c68",
   "metadata": {
    "id": "f80b9f11-e31a-41a5-b8e6-813bf37d7c68"
   },
   "source": [
    "### 3.0 Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a768a-a2b2-4fec-a5ea-af30c20dbfd9",
   "metadata": {
    "id": "575a768a-a2b2-4fec-a5ea-af30c20dbfd9"
   },
   "source": [
    "Just to be alligned, we'll explain here the following terms:\n",
    "\n",
    "* **expert** - A specialized model or sub-model intended to handle a specific subset of data or tasks. It can be assumed as a small neural network (Usually some few neurons) which is a sub part of a layer.\n",
    "* **product key** - A mechanism to enable efficient and effective matching between input data and the best-suited experts to process that data in a vast and diverse pool of potential experts. Every product key is a vector which represents the features or aspects of inputs that the expert is particularly suited to handle.\n",
    "* **feed forward layer** - A layer which the information is passed in one direction, from input to output, without any loops or feedback within the layer itself, and with no backward propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae322a-6a7f-4b44-b5e4-57bc15d704a5",
   "metadata": {
    "id": "ceae322a-6a7f-4b44-b5e4-57bc15d704a5"
   },
   "source": [
    "### 3.1 Introduction and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5675abf-bf46-4439-9efb-4f57ee397fd3",
   "metadata": {
    "id": "d5675abf-bf46-4439-9efb-4f57ee397fd3"
   },
   "source": [
    "- Based on the following paper: https://arxiv.org/pdf/2407.04153v1.\n",
    "- The main idea here is the innovation of the sparse mixture-of-experts (MoE) model architectures using PEER (parameter efficient expert retrieval) layer.\n",
    "- The PEER layer includes a vast number of tiny expert (over a million) and solves one of the main issues of the traditional models:\n",
    "  - In traditional MoE models, the feedforward layers (FFW) have a linear increase in computational costs and activation memory as the hidden layer width grows.\n",
    "  - However, in PEER, by enabling efficient utilization of a massive number of experts, it can support a further scaling of transformer models while maintaining almost the same computational efficiency.\n",
    "  - It uses the product key technique for sparse retrieval.\n",
    "\n",
    "The main contributions of this work are:\n",
    "- ***Exploration of Extreme MoE Setting:*** Unlike previous MoE research, this work investigates the under-explored case of numerous tiny experts.\n",
    "- ***Learned Index Structure for Routing:*** Demonstrating for the first time that a learned index structure\n",
    "(Kraska et al., 2018) can efficiently route to over a million experts.\n",
    "- ***New Layer Design:*** Combining product key routing with single-neuron experts, we introduce the\n",
    "PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its major efficiency compared to MoE architectures in previous researches.\n",
    "- ***Comprehensive Ablation Studies:*** Investigating the impact of different design choices of PEER\n",
    "such as number of experts, active parameters, number of heads and query batch normalization on\n",
    "language modeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de9cd4-23cb-4084-9c74-a7fbe21d5623",
   "metadata": {
    "id": "14de9cd4-23cb-4084-9c74-a7fbe21d5623"
   },
   "source": [
    "### 3.2 Architecture and mathematical equalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6aef6-c0ce-4246-8821-0ab1ca0db463",
   "metadata": {
    "id": "2bf6aef6-c0ce-4246-8821-0ab1ca0db463"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/Peer_Layer.png?raw=1\" alt=\"Peer_Layer\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cc5dd-7fe5-4a11-80c4-807d84608e65",
   "metadata": {
    "id": "f19cc5dd-7fe5-4a11-80c4-807d84608e65"
   },
   "source": [
    "The following is an illustration of the PEER layer:\n",
    "* A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers\n",
    "* Given the state vector x from the previous layer, a query\n",
    "network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the\n",
    "router scores and to retrieve the top $k$ experts $e_1$, ..., $e_k$\n",
    "* After the retrieved experts make their predictions\n",
    "ei(x), their outputs are linearly combined using the softmax-normalized router scores as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132de35-4f8d-40b5-b873-7f9a41bffd7d",
   "metadata": {
    "id": "c132de35-4f8d-40b5-b873-7f9a41bffd7d"
   },
   "source": [
    "Formally, a PEER layer is a function $f:{R^n}\\rightarrow{R^m}$ that consists of three parts:\n",
    "* A pool of $N$ experts $E:=\\{{e_i}\\}_{i=1}^{N}$ where each expert $e_i:{R^n}\\rightarrow{R^m}$ shares the same signature as $f$\n",
    "* A matching set of $N$ produect keys: $K:=\\{{k_i}\\}_{i=1}^{N} \\subset R^d$ ($d$ is the dimension of the vectors)\n",
    "* A query network $q:{R^n}\\rightarrow{R^d}$ that maps the input vector $x\\in{R^n}$ to a query vector $q(x)$\n",
    "\n",
    "The layer output is done by the following three steps:\n",
    "1. Retrieving the top $k$ experts: The experts who their corresponding product keys have the highest inner products with the query $q(x)$: $I=T_k(\\{q(x)^Tk_i\\})_{i=1}^{N}$ while $T_k$ is the top $k$ operator\n",
    "2. Actication: Applying nonlinear activations (such as softmax or sigmoid) to the query-key inner products of these\n",
    "top $k$ experts to obtain the router scores: $g_i(x) = s(q(x)^Tk_i)$\n",
    "3. Output: Computing the output by linearly combining the expert outputs weighted by the router scores: $f(x) = \\sum_{{i}\\in{I}}g_i(x)e_i(x)$.\n",
    "\n",
    "How are the most suitable $k$ experts are actually chosen?\n",
    "1. Query vectors Generation: A set of query vectors is generated from the input vectors. These query vectors are created specifically for the task of selecting the appropriate experts. Each product in the \"Product of Experts\" approach has its own query vectors.\n",
    "2. Scoring of query vectors: Each query vector is then scored based on its compatibility with the product keys associated with the available experts. This scoring is typically done using a similarity measure, such as a dot product.\n",
    "3. Top-k Selection: The scores determine the relevance or importance of each expert for the given input vector x. The k highest scoring experts (i.e., those with the keys having the highest similarity scores) are selected as the active experts for that particular input\n",
    "\n",
    "Product key retrieval - Efficiency:\n",
    "* Since $N$ can be a huge number $(N \\geq 10^6)$, we don't want to compute it by the naive way\n",
    "* Instead of using $N$ independent $d$-dimensional vectors as our product keys $k_i$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys $C, C' \\subset R^{\\frac{d}{2}}$, $|C|=|C'|=\\sqrt{N}$\n",
    "* So in total $K = {\\{[_{c'}^{c}]|c\\in{C}, c'\\in{C'}\\}}$\n",
    "* Hence we choose $N$ to be a perfect square and $d$ to be an even number\n",
    "* Instead of comparing $q(x)$ to all $N$ keys in $K$ and selecting the top $k$ matches, we split the query vector $q(x)$ into two subqueries $q1$ and $q2$ (Each one of them is d/2 dimensional) and apply the top $k$ operations to the inner products between the sub-queries and sub-keys\n",
    "respectively\n",
    "* This results in a set of $k^2$ candidate keys, and it is mathematically guaranteed that the $k$ most similar keys to $q(x)$ from $K$ are in this candidate set so we can simply apply the top-k operator again to these $k^2$ inner products to get the top $k$ matching keys from the original set of product keys $K$\n",
    "* Naive way runtime complexity: $O(Nd)$\n",
    "* Efficient way runtime complexity: $O((\\sqrt{N}+k^2)d)$\n",
    "\n",
    "\n",
    "Parameter Efficient Experts and Multi-Head Retrieval:\n",
    "* In other MoE architectures, the hidden layer of each expert is set to the same size as other FFW layers (A few and large experts)\n",
    "* In PEER, every expert is a one hidden layer with a single neuron $e_i(x) := \\sigma(u_i^Tx)v_i$ while $\\sigma$ is a non-linear activation function\n",
    "* Instead of making the size of individual experts different, we are using a multihead retrieval of $h$ independent query networks instead of one, each computes its own query and retrieves a separate set of $k$ experts: $f(x) := \\sum_{i=1}^{h}f^i(x) = \\sum_{i=1}^{h}\\sum_{j \\in I^i} g^j(x)e^j(x)$\n",
    "\n",
    "What are the main benefits of Multi-Head retrieval ?\n",
    "* Parralelism: Multiple heads can process their respective inputs in parallel, significantly improving computational efficiency and speed when dealing with complex or large datasets. In addition, scaling is much easier using multiple heads\n",
    "* Specialization and comprehensive processing: Each head can specialize in a different aspect or feature of the data, enabling the model to handle a broader array of input complexities. For instance, one head may specialize in low-frequency features of the data, while another focuses on high-frequency components. This allows the system to capture more comprehensive information from the input, as different heads may pick up on different signals or patterns that a single expert (which is a single neuron) might miss.\n",
    "* Improvment of overfitting and noise: Different heads may learn to represent different parts of the data space, which can help reduce the risk of overfitting to specific idiosyncrasies in the training data, and to be less sensitive to noise or variability in specific parts of the data.\n",
    "  \n",
    "How can an expert be efficient if it has just a single Neuron ?\n",
    "1. During training, the neuron's weights are adjusted to respond preferentially to specific patterns in the data. For instance, a neuron could be tuned to react strongly to certain spatial features in an image or a text.\n",
    "2. Usually, a single neuron can effectively perform binary classification based on whether the input meets a certain condition (e.g., whether the aggregated weighted sum reaches a particular threshold). Usually a single neuron has a specialized feature.\n",
    "3. Its output would be combined with that of other experts (which could be either neurons or more complex networks), each handling different features or aspects of the data. This combination would typically be a weighted sum or another associative operation, guiding the final decision or output of the PEER layer.\n",
    "4. The multi-head method provided above makes the work with single Neurons possible and effective despite their size\n",
    "\n",
    "Hyperparameters and loss function:\n",
    "* There are three main hyperparameters to a standard MoE layer:\n",
    "  1. $P$ - Total number of parameters\n",
    "  2. $P_{active}$ - Total number of active parameters per token\n",
    "  3. $P_{expert}$ - The size of a single expert\n",
    "* Two more important terms:\n",
    "  1. $D$ - Number of training tokens\n",
    "  2. $G := \\frac{P_{active}}{P_{experts}}$ - Number of active experts\n",
    "* Now the loss function is $L(P, D, G) = c + (\\frac{g}{G^\\lambda} + a)\\frac{1}{P^\\alpha} + \\frac{b}{D^\\beta}$ ($a, b, c, g, \\alpha, \\beta, \\lambda$ are constants)\n",
    "* We would like to scale the total number of parameters, of experts and of the tokens, but NOT the number of active parameters, since the number of the active parameters affects the computational and runtime costs\n",
    "* Since we want to increase the number of experts, we need to decrease the size of each expert if we do not increase the number of active parameters. Hence we need a large number of small experts\n",
    "* In PEER, we set $P_{experts}$ to 1 and $P_{active}$ is the number of retrieval heads multiplied by the number of experts retrieved per head which is $hk$. Hence, in PEER layer the number of active experts is $G = hk$\n",
    "\n",
    "Below is a Pseudo Code sample of a PEER Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9e754-31da-4fc7-b5b6-6f7c965c6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import math\n",
    "from einops import einsum\n",
    "from tqdm import tqdm \n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class PEER(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        heads=8,\n",
    "        num_experts=1_000_000,\n",
    "        num_experts_per_head=16,\n",
    "        activation=nn.GELU,\n",
    "        dim_key=None,\n",
    "        product_key_topk=None,\n",
    "        separate_embed_per_head=False,\n",
    "        pre_rmsnorm=False,\n",
    "        dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Here the norm is the torch identity of each torch\n",
    "        self.norm = RMSNorm(dim) if pre_rmsnorm else nn.Identity()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.separate_embed_per_head = separate_embed_per_head\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # There are some sets of experts as the number of heads\n",
    "        num_expert_sets = heads if separate_embed_per_head else 1\n",
    "\n",
    "        # Embedding layers storing the down /up projection weights of all experts\n",
    "        self.weight_down_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "        self.weight_up_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "\n",
    "        # The activation method is GELU (Gaussian Error Linear Unit)\n",
    "        self.activation = activation()\n",
    "\n",
    "        # As told, the number of experts must be a perfect square\n",
    "        assert (num_experts ** 0.5).is_integer(), '`num_experts` needs to be a square'\n",
    "\n",
    "        # As told, the number of dimension must be even number\n",
    "        assert (dim % 2) == 0, 'feature dimension should be divisible by 2'\n",
    "\n",
    "        # The dimension of each one of the product keys is d/2\n",
    "        dim_key = dim_key if exists(dim_key) else dim // 2\n",
    "\n",
    "        # The number of keys is the the root square of the number of experts\n",
    "        self.num_keys = int(num_experts ** 0.5)\n",
    "\n",
    "        # A linear layer of the query vectors\n",
    "        self.to_queries = nn.Sequential(\n",
    "            nn.Linear(dim, dim_key * heads * 2, bias=False),\n",
    "            Rearrange('b n (p h d) -> p b n h d', p=2, h=heads)\n",
    "        )\n",
    "\n",
    "        # The product key topK is the number of experts per head\n",
    "        self.product_key_topk = product_key_topk if exists(product_key_topk) else num_experts_per_head\n",
    "        self.num_experts_per_head = num_experts_per_head\n",
    "\n",
    "        # There are a total of h * root square of N keys and each one of them has a dimension of d/2\n",
    "        self.keys = nn.Parameter(torch.randn(heads, self.num_keys, 2, dim_key))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In our case, keep the input vectors x as is\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Build query vectors q(x) from x\n",
    "        queries = self.to_queries(x)\n",
    "\n",
    "        ###\n",
    "        # Retrieve the weights of the top matching experts using product keys\n",
    "        # indices and scores have the shape ’bnhk ’, where h is the number of heads\n",
    "        ###\n",
    "        # Einsum is a matrix-multicipation describing the dimension of each matrix X, Y -> Z <=> X*Y = Z\n",
    "        sim = einsum(queries, self.keys, 'p b n h d, h k p d -> p b n h k')\n",
    "        (scores_x, scores_y), (indices_x, indices_y) = [s.topk(self.product_key_topk, dim=-1) for s in sim]\n",
    "        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n",
    "        all_indices = indices_x.unsqueeze(-1) * self.num_keys + indices_y.unsqueeze(-2)\n",
    "        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n",
    "        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n",
    "        scores, pk_indices = all_scores.topk(self.num_experts_per_head, dim=-1)\n",
    "        indices = all_indices.gather(-1, pk_indices)\n",
    "\n",
    "        # Is false in our use case\n",
    "        if self.separate_embed_per_head:\n",
    "            head_expert_offsets = torch.arange(self.heads, device=x.device) * self.num_experts\n",
    "            indices = indices + head_expert_offsets.view(1, 1, -1, 1)\n",
    "\n",
    "        # Setup the down / up projection weights of all experts based on the indices\n",
    "        weights_down = self.weight_down_embed(pk_indices)\n",
    "        weights_up = self.weight_up_embed(pk_indices)\n",
    "\n",
    "        ####\n",
    "        # Compute weighted average of expert outputs\n",
    "        ####\n",
    "\n",
    "        # Matrix multicipation of the expert outputs and the bottom projected weights \n",
    "        x = einsum(x, weights_down, 'b n d, b n h k d -> b n h k')\n",
    "\n",
    "        # GELU Activation\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # Zero some of the outputs with a probability of \"dropout\" which is 0 in our case (Do not reset outputs)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Softmax compution\n",
    "        x = x * F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Matrix multicipation of the expert outputs and the upper projected weights \n",
    "        x = einsum(x, weights_up, 'b n h k, b n h k d -> b n d')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012373d-bae6-4434-a232-692cf8c59c60",
   "metadata": {
    "id": "5012373d-bae6-4434-a232-692cf8c59c60"
   },
   "source": [
    "### 3.3 Results of major experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5015af-e108-4a66-8774-f5ae9a99059f",
   "metadata": {
    "id": "cf5015af-e108-4a66-8774-f5ae9a99059f"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/Peer_Experiment.png?raw=1\" alt=\"PEER_Experiment\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421858d6-0557-42b3-b9d5-8e35cdf6a3a2",
   "metadata": {
    "id": "421858d6-0557-42b3-b9d5-8e35cdf6a3a2"
   },
   "source": [
    "The following is a graph of the train loss per batch of their training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621baeb-678b-4f57-921c-d7ccc874985a",
   "metadata": {
    "id": "9621baeb-678b-4f57-921c-d7ccc874985a"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/6e18_6e19_FLOPs.png?raw=1\" alt=\"6e18_6e19_FLOPs\" width=\"700\" height=\"500\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/Varying_N_hk.png?raw=1\" alt=\"Varying_N_hk\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab50e99-1c66-4491-84d1-2a400f6cd1ac",
   "metadata": {
    "id": "8ab50e99-1c66-4491-84d1-2a400f6cd1ac"
   },
   "source": [
    "* The following graphs in the upper side shows the perplexity per number of parameters for 2e19 (more efficient computation) and 2e18 (less efficient computation) FLOP's, comparing different MoE models:\n",
    "  * Dataset is $C4$\n",
    "  * Total number of experts $N$ is $1024^2$\n",
    "  * The number of active experts is $128$ for Dense, PEER and MOE and $256$ memories for PKM\n",
    "  * It can be shown that PEER gives the lower perplexity and with the largest number of parameters\n",
    "* The graph in the left-bottom side shows the perplexity per number of parameters of PEER models, comparing different number of total experts $N$\n",
    "  * The size of the active experts $hk$ is always $128$\n",
    "  * Dataset is $C4$\n",
    "* The graph in the right-bottom side shows the perplexity per number of $h$ independent query networks (Active experts)\n",
    "  * Total number of experts $N$ is always $1024^2$\n",
    "  * Dataset is $C4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4e99a-3ae3-46d6-a9c6-26c578c83c20",
   "metadata": {
    "id": "aaa4e99a-3ae3-46d6-a9c6-26c578c83c20"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/PEER_Experiment_Table.png?raw=1\" alt=\"PEER_Experiment_Table\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2321f72-dc8d-4f60-8ccd-ebe239dec7bd",
   "metadata": {
    "id": "d2321f72-dc8d-4f60-8ccd-ebe239dec7bd"
   },
   "source": [
    "The following table compares the perplexity per model (rows) and per dataset (columns):\n",
    "  * Total number of experts $N$ is $1024^2$\n",
    "  * The number of active experts is $128$ for Dense, PEER and MOE and $256$ memories for PKM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce778b-b88f-481a-b266-2a322a9441c5",
   "metadata": {
    "id": "f2ce778b-b88f-481a-b266-2a322a9441c5"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/BatchNormTable.png?raw=1\" alt=\"BatchNormTable\" width=\"600\" height=\"600\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/BatchNormGraph.png?raw=1\" alt=\"BatchNormGraph\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf2edd-b70a-433f-b810-d85a38fcf496",
   "metadata": {
    "id": "46cf2edd-b70a-433f-b810-d85a38fcf496"
   },
   "source": [
    "The following table and graph shows the perplexity per number of experts $N$ (In the graph the number of experts $N$ is $1M$) and using two scenarios:\n",
    "* With no query batch normalization\n",
    "* With query batch normalization\n",
    "\n",
    "These are the following details of experiments:\n",
    "* Dataset is $C4$\n",
    "* Model is PEER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b5ff6-5f7c-42fa-ace0-fc9eea195c76",
   "metadata": {
    "id": "ee6b5ff6-5f7c-42fa-ace0-fc9eea195c76"
   },
   "source": [
    "### 3.4 Hands-on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a63e7d-904a-496b-a20f-0a7d0171ece7",
   "metadata": {
    "id": "c3a63e7d-904a-496b-a20f-0a7d0171ece7"
   },
   "outputs": [],
   "source": [
    "!pip install datasets --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JqhTlTihNRI8",
   "metadata": {
    "id": "JqhTlTihNRI8"
   },
   "outputs": [],
   "source": [
    "%rm -rf moe-llm-presentation\n",
    "!git clone https://github.com/stavco9/moe-llm-presentation\n",
    "%cd moe-llm-presentation/peer_main\n",
    "%ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c681d514-7d3a-4d56-9edf-3b110e743bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lEGeog9d9l66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEGeog9d9l66",
    "outputId": "7f2f7939-35dc-4a8f-f5f1-26dffbd0f97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': '50257', 'dim': '64', 'num_layers': '4', 'num_heads': '4', 'num_experts': '16384', 'top_k': '8', 'batch_size': '6', 'num_epochs': '6', 'learning_rate': '0.0001', 'dataset': 'Salesforce/wikitext'}\n",
      "Loading pretrained GPT2 tokenizer transformer\n",
      "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 212kB/s]\n",
      "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.59MB/s]\n",
      "merges.txt: 100% 456k/456k [00:00<00:00, 1.05MB/s]\n",
      "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 1.99MB/s]\n",
      "config.json: 100% 665/665 [00:00<00:00, 6.02MB/s]\n",
      "Finished loading pretrained GPT2 tokenizer transformer\n",
      "Initalizing PEER model\n",
      "Finished initalizing PEER model\n",
      "Loading datasets\n",
      "README.md: 100% 10.5k/10.5k [00:00<00:00, 43.2MB/s]\n",
      "test-00000-of-00001.parquet: 100% 733k/733k [00:00<00:00, 75.7MB/s]\n",
      "train-00000-of-00002.parquet: 100% 157M/157M [00:00<00:00, 350MB/s]\n",
      "train-00001-of-00002.parquet: 100% 157M/157M [00:00<00:00, 506MB/s]\n",
      "validation-00000-of-00001.parquet: 100% 657k/657k [00:00<00:00, 409MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 461422.14 examples/s]\n",
      "Generating train split: 100% 1801350/1801350 [00:01<00:00, 908135.61 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 632899.23 examples/s]\n",
      "Filter: 100% 1801350/1801350 [00:04<00:00, 402742.27 examples/s]\n",
      "Filter: 100% 3760/3760 [00:00<00:00, 315582.08 examples/s]\n",
      "Finished loading datasets\n",
      "Number of parameters: 23703808\n",
      "Epoch Training 1/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.88it/s]\n",
      "Epoch Validation 1/6\n",
      "100% 411/411 [00:17<00:00, 22.95it/s]\n",
      "Epoch 1/6, Train Loss: 1.8483, Val Loss: 1.4844, Val Perplexity: 4.4124\n",
      "Epoch Training 2/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.88it/s]\n",
      "Epoch Validation 2/6\n",
      "100% 411/411 [00:17<00:00, 23.00it/s]\n",
      "Epoch 2/6, Train Loss: 1.3455, Val Loss: 1.8030, Val Perplexity: 6.0680\n",
      "Epoch Training 3/6\n",
      "100% 50000/50000 [1:24:22<00:00,  9.88it/s]\n",
      "Epoch Validation 3/6\n",
      "100% 411/411 [00:17<00:00, 22.99it/s]\n",
      "Epoch 3/6, Train Loss: 1.2391, Val Loss: 1.8606, Val Perplexity: 6.4276\n",
      "Epoch Training 4/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.88it/s]\n",
      "Epoch Validation 4/6\n",
      "100% 411/411 [00:17<00:00, 23.03it/s]\n",
      "Epoch 4/6, Train Loss: 1.1785, Val Loss: 1.8852, Val Perplexity: 6.5876\n",
      "Epoch Training 5/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.87it/s]\n",
      "Epoch Validation 5/6\n",
      "100% 411/411 [00:17<00:00, 22.88it/s]\n",
      "Epoch 5/6, Train Loss: 1.1468, Val Loss: 1.8965, Val Perplexity: 6.6626\n",
      "Epoch Training 6/6\n",
      "100% 50000/50000 [1:24:22<00:00,  9.88it/s]\n",
      "Epoch Validation 6/6\n",
      "100% 411/411 [00:17<00:00, 23.05it/s]\n",
      "Epoch 6/6, Train Loss: 1.1283, Val Loss: 1.8989, Val Perplexity: 6.6784\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8192\"\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 50257  # GPT-2 tokenizer vocab size\n",
    "dim = 64 # The dimension of each query vector d\n",
    "num_layers = 4 # The number of peer layers\n",
    "num_heads = 4 # The number of heads h\n",
    "num_experts = 128 * 128 # The total number of experts N\n",
    "top_k = 8 # The top k to use for each head\n",
    "batch_size = 6 # The batch size of each input\n",
    "num_epochs = 6 # Total number of epochs\n",
    "learning_rate = 1e-4\n",
    "dataset = 'Salesforce/wikitext'\n",
    "\n",
    "!torchrun --nproc_per_node=1 --nnodes=1 main.py --vocab-size={vocab_size} --dim={dim} \\\n",
    "  --num-layers={num_layers} --num-heads={num_heads} --num-experts={num_experts} --top-k={top_k} --batch-size={batch_size} \\\n",
    "  --num-epochs={num_epochs} --learning-rate={learning_rate} --dataset={dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "827308a8-6d47-40a2-b514-8330dc28c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.array([1.8483, 1.3455, 1.2391, 1.1785, 1.1468, 1.1283])\n",
    "validation_losses = np.array([1.4844, 1.8030, 1.8606, 1.8852, 1.8965, 1.8989])\n",
    "validation_perplexity = np.array([4.4124, 6.0680, 6.4276, 6.5876, 6.6626, 6.6784])\n",
    "episodes = np.arange(1, num_epochs + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd96c64-40ba-4b28-997c-a0dc41e733da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/04/b5_9lzx5797dzscx6rwj46k80000gr/T/ipykernel_7019/665747474.py:4: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-gD\" (-> color='g'). The keyword argument will take precedence.\n",
      "  plt.plot(episodes, train_losses, '-gD', color = \"orange\", label='Train Loss')\n",
      "/var/folders/04/b5_9lzx5797dzscx6rwj46k80000gr/T/ipykernel_7019/665747474.py:5: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-gD\" (-> color='g'). The keyword argument will take precedence.\n",
      "  plt.plot(episodes, validation_losses, '-gD', color = \"blue\", label='Valid Loss')\n",
      "/var/folders/04/b5_9lzx5797dzscx6rwj46k80000gr/T/ipykernel_7019/665747474.py:6: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-gD\" (-> color='g'). The keyword argument will take precedence.\n",
      "  plt.plot(episodes, validation_perplexity, '-gD', color = \"purple\", label='Valid Perplexity')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Train & Valid loss & Perplexity\") \n",
    "plt.xlabel(\"Episode\") \n",
    "plt.ylabel(\"Loss / Perplexity\") \n",
    "plt.plot(episodes, train_losses, '-gD', color = \"orange\", label='Train Loss')\n",
    "plt.plot(episodes, validation_losses, '-gD', color = \"blue\", label='Valid Loss')\n",
    "plt.plot(episodes, validation_perplexity, '-gD', color = \"purple\", label='Valid Perplexity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61381284-377f-4d5c-af5e-897a4d37efa9",
   "metadata": {},
   "source": [
    "We can see here that the train loss is improving while valid loss / perplexity are getting worse. This is a sign on overfitting. One of the possible reasons of that is the fact we decreased a lot every one of the parameters in order to get the model trained with a realible amount of time, including the number of layers, and also the total number of experts and the number of the k-selected experts"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
