{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135ae95f",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stavco9/moe-llm-presentation/blob/main/presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28700274-faf1-4b4a-ba3c-159a77d81563",
   "metadata": {
    "id": "28700274-faf1-4b4a-ba3c-159a77d81563"
   },
   "source": [
    "# Mixture of experts Language Model presentation\n",
    "\n",
    "## Noam Delbari & Stav Cohen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6",
   "metadata": {
    "id": "b88c93a4-e500-4480-a9fe-16e50791a9f6"
   },
   "source": [
    "## 1 Background:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed0608-413e-48b7-a512-3b3cc884ca26",
   "metadata": {},
   "source": [
    "### 1.1 Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe019de3-0300-4285-9dcd-6182400f5e56",
   "metadata": {
    "id": "fe019de3-0300-4285-9dcd-6182400f5e56"
   },
   "source": [
    "A **Transformer** is an *encoder–decoder* sequence‑to‑sequence model that performs all its\n",
    "computation with stacked **self‑attention** layers and small position‑wise feed‑forward networks (FFNs),\n",
    "with **no recurrence or convolution**. This design lets every token interact with every other token in\n",
    "one parallelizable matrix operation, unlocking massive throughput on modern accelerators.\n",
    "\n",
    "###  Standard Transformer\n",
    "\n",
    "| Sub-layer | Inputs | Outputs | Purpose |\n",
    "|-----------|--------|---------|---------|\n",
    "| **Self-Attention** | A sequence of token embeddings $X\\!\\in\\!\\mathbb{R}^{L\\times d}$ | A new sequence $Z\\!\\in\\!\\mathbb{R}^{L\\times d}$ | Mix information across all positions using *scaled dot-product attention*. |\n",
    "| **Add + LayerNorm** | Residual input + sub-layer output | Normalised sum | Stabilise training and preserve gradients (residual path). |\n",
    "| **Feed-Forward Network (FFN)** | Each token independently | Transformed tokens | Non-linear feature expansion: two linear maps with GELU/ReLU in between. |\n",
    "| **Positional Encoding / Bias** | Token index | Vector added to embeddings or into attention logits | Inject order information, because plain attention is permutation-invariant. |\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/transformer.png\" width=\"300\"/>\n",
    "  <figcaption><b>Figure 1</b> – Transformer architecture.</figcaption>\n",
    "</figure>\n",
    "\n",
    "- **Encoder:** builds a *bidirectional* representation of the source (Self-Attn + FFN).  \n",
    "- **Decoder:** generates the target *left-to-right*, consulting both its own history (causal Self-Attn) and the encoder (Cross-Attn).  \n",
    "- **Residual highways** keep gradients flowing; **LayerNorm** keeps scales stable; **Feed-Forward sub-nets** inject non-linearity.  \n",
    "- The tensor shape ($L \\times d$) stays constant; **contextual and abstract information** accumulates at each hop.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Embedding Stage\n",
    "\n",
    "Raw word-pieces are mapped to $d$-dimensional vectors and receive a positional signal (either sinusoidal values or a learned bias).\n",
    "\n",
    "**Result:** “Naked” token embeddings that know their position but nothing yet about their neighbours.\n",
    "\n",
    "#### 2. Encoder Conveyor (× N layers)\n",
    "\n",
    "For *N* identical encoder layers the vectors ride through the same three-step processing loop:\n",
    "\n",
    "1. **Self-Attention**  \n",
    "   Every token points a spotlight at all other tokens, gathers a weighted sum of their features, and replaces its own vector with that mixture.  \n",
    "   Instantly injects **global context**—long-range dependencies can be captured in a single hop.\n",
    "\n",
    "2. **Residual & LayerNorm**  \n",
    "   Add the mixer’s output back to the original vector (so nothing is forgotten), then apply LayerNorm (so activations stay in a healthy range).\n",
    "\n",
    "3. **Feed-Forward Refiner**  \n",
    "   Apply a two-layer MLP with GELU/ReLU per token—no cross-token interaction.  \n",
    "   Acts as a non-linear feature transformer, increasing capacity.\n",
    "\n",
    "After a second residual+norm, the token moves to the next layer carrying both its original identity and all the context it just absorbed.\n",
    "\n",
    "> **Outcome:** After *N* laps, each encoder vector is a rich, position-aware summary of the entire source sentence. The tensor shape ($L \\times d$) never changes—only the **information content** inside each vector grows.\n",
    "\n",
    "#### 3. Decoder Conveyor (× N layers)\n",
    "\n",
    "The decoder also loops *N* times, but each layer adds a “side-road”:\n",
    "\n",
    "1. **Causal Self-Attention**  \n",
    "   Tokens attend only to positions $\\le t$ (auto-regressive).  \n",
    "   **Result:** each position knows everything generated so far, but not the future.\n",
    "\n",
    "2. **Residual & LayerNorm**\n",
    "\n",
    "3. **Cross-Attention Bridge**  \n",
    "   *Queries* = decoder state; *Keys/Values* = frozen encoder outputs.  \n",
    "   Aligns the partially generated target with relevant source content.\n",
    "\n",
    "4. **Residual & LayerNorm**\n",
    "\n",
    "5. **Feed-Forward Refiner**  \n",
    "   Same per-token MLP as in the encoder.\n",
    "\n",
    "> **Loop:** Tokens accrue context from both their left-hand history and the full source sentence over *N* layers.\n",
    "\n",
    "#### 4. Output Projection\n",
    "\n",
    "After the final decoder layer each vector passes through a shared linear map + softmax to produce a probability distribution over the next vocabulary token.\n",
    "\n",
    "\n",
    "####   Why does it work  \n",
    "\n",
    "* **Parallelism** – all tokens processed simultaneously (unlike RNNs).  \n",
    "* **Long-range dependencies** – any token can attend to any other in one hop.  \n",
    "* **Expressiveness** – stacking layers lets the model build hierarchical features.  \n",
    "* **Speed** – matrix multiplications map well to GPUs/TPUs.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04",
   "metadata": {
    "id": "4b16ecc2-ad9d-4f1c-9bfe-788abe1f2d04"
   },
   "source": [
    "### 1.2 Mixture-of-Experts (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a",
   "metadata": {
    "id": "2d106ada-a163-49f4-82af-ba3b2dd9bb2a"
   },
   "source": [
    "Mixure-of-Experts (MoE) is a machine learning technique where multiple expert networks (learners) are used to divide a problem space into dedicated regions. Rather than a big one network that makes all the required tasks, it's splitted to many dedicated experts which each one of them makes a dedicated task.\n",
    "\n",
    "#### A Brief History of MoEs\n",
    "According to the following paper: https://huggingface.co/blog/moe <br>\n",
    "* The roots of MoEs come from the 1991 paper Adaptive Mixture of Local Experts.\n",
    "* The main idea was to have a supervised procedure for a system composed of separate networks, each handling a different subset of the training cases.\n",
    "* Each separate network, or expert, specializes in a different region of the input space.\n",
    "\n",
    "Between 2010-2015, two different research areas contributed to later MoE advancement:\n",
    "* Experts as components: In the traditional MoE setup, the whole system includes a gating network and multiple experts\n",
    "* MoEs as the whole model have been explored in ***SVMs***, ***Gaussian Processes***, and other methods\n",
    "* The work by Eigen, Ranzato, and Ilya explored MoEs as components of deeper networks: This allows having MoEs as layers in a multilayer network, making it possible for the model to be both large and efficient simultaneously.\n",
    "* Conditional Computation: Traditional networks process all input data through every layer. In this period, Yoshua Bengio researched approaches to dynamically activate or deactivate components based on the input token.\n",
    "* These works led to exploring a mixture of experts in the context of NLP.\n",
    "* Concretely, Shazeer et al. (2017, with “et al.” including Geoffrey Hinton and Jeff Dean, Google’s Chuck Norris) scaled this idea to a 137B LSTM (the de-facto NLP architecture back then, created by Schmidhuber) by introducing sparsity, allowing to keep very fast inference even at high scale.\n",
    "* This work focused on translation but faced many challenges, such as high communication costs and training instabilities.\n",
    "\n",
    "#### Some terms\n",
    "1. ***Expert***\n",
    "A small and specialized model which got trained for a particular area. It can be a neural network, decision tree, or other algorithm. In our case experts are small neural networks.\n",
    "\n",
    "3. ***A Mixture of Experts (MoE) model***\n",
    "A model that combines the predictions of multiple experts to solve complex problems.\n",
    "- Each expert is trained on a specific domain or task, and a \"gating network\" or \"router\" selects the most appropriate experts for a given input.\n",
    "\n",
    "3. ***\"gating network\" / \"router\"***\n",
    "A component (a tiny linear layer) in the large model that determines which experts should be activated for a particular input. It's also trained along with the experts\n",
    "\n",
    "#### What do we achieve from that ?\n",
    "The main benefit of the MoE architecture is that it enables large-scale models, even those comprising many billions of parameters, to reduce computation costs during pre-training and achieve faster performance during evaluation time.\n",
    "\n",
    "#### How does it work ?\n",
    "It reaches it's major benefit by selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task.\n",
    "\n",
    "#### An illustration of a standard MoE network\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/moe.png?raw=1\" alt=\"Peer_Layer\" width=\"400\" height=\"400\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/01_moe_layer.png?raw=1\" alt=\"Peer_Layer\" width=\"500\" height=\"600\">\n",
    "</figure>\n",
    "In the left image, the red experts are those who are active\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e70a8c-89fa-4649-83ba-0e205befe346",
   "metadata": {
    "id": "b9e70a8c-89fa-4649-83ba-0e205befe346"
   },
   "source": [
    "## 2 Switch transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d7d88",
   "metadata": {},
   "source": [
    "## 2.1 Introduction:\n",
    "\n",
    "Large language models (LLMs) have achieved striking gains by growing from millions\n",
    "to billions of parameters—yet *dense* scaling makes **every** parameter participate\n",
    "in **every** forward-pass.  \n",
    "Compute (FLOPs), memory traffic, and wall-time therefore grow linearly with model\n",
    "size, and the trillion-parameter frontier strains even the largest clusters.\n",
    "\n",
    "**Mixture-of-Experts (MoE)** layers offer a different path: *conditional\n",
    "computation*.  \n",
    "A lightweight *gating network* selects **one** or **few** specialized “experts”\n",
    "(MLPs) per token, so only a *subset* of parameters is active each step.\n",
    "Switch Transformers (Fedus *et&nbsp;al.*, 2022) refine this idea to make it practical\n",
    "at unprecedented scale.\n",
    "\n",
    "### Why previous MoE attempts struggled  \n",
    "\n",
    "| Bottleneck in earlier MoE work | What it means | Switch Transformer’s remedy |\n",
    "| :-- | :-- | :-- |\n",
    "| **Unstable top-k routing** (k > 1) | When every token is split across *k* experts (k = 2,4…), the soft mixture may starve some experts of gradient signal → divergence in very deep models. | **k = 1 “switch” routing**: each token is sent to exactly one expert chosen by `argmax` over gate logits. This keeps gradients intact and halves the routing tensor size. |\n",
    "| **Cross-device communication** | Prior systems sliced *one* expert across many GPUs/TPUs → every step required an All-to-All of hidden states. | **Expert-parallel layout**: each device *owns* one whole expert. Tokens are grouped by destination expert, transferred *once*, processed locally, then regrouped—minimising traffic. |\n",
    "| **Token imbalance (hot-spot experts)** | Popular tokens (e.g., punctuation) can overload a few experts, leaving others idle and blowing up memory. | **Auxiliary load-balancing loss**: penalises correlation between (i) fraction of tokens routed to expert *i* and (ii) gate probability mass on expert *i*.  <br>$$ L_{\\text{aux}} = \\alpha\\,N \\sum_{i=1}^{N} f_i\\,P_i \\quad\\text{(Eq.\\;4)} $$ |\n",
    "\n",
    "*Definitions*  \n",
    "* **Expert** – an independent feed-forward sub-network (here, a position-wise MLP).  \n",
    "* **Gating network** – a tiny linear layer that produces *N* logits per token.  \n",
    "* **Routing** – assigning tokens to experts based on gate probabilities.  \n",
    "* **FLOPs / token** – floating-point operations needed for one token’s forward-pass.\n",
    "\n",
    "---\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "This paper explicitly addresses several critical research questions:\n",
    "\n",
    "1. **Efficiency vs. Model Capacity:**  \n",
    "   *Does increasing model size through sparse routing (more experts) consistently improve language model performance (perplexity) without proportional computational cost?*\n",
    "\n",
    "2. **Training Stability:**  \n",
    "   *Can models at trillion-parameter scale be stably trained using lower-precision arithmetic (like bfloat16)?*\n",
    "\n",
    "3. **Downstream Generalization:**  \n",
    "   *Do improvements obtained during language modeling pre-training generalize to downstream tasks (e.g., QA, summarization)?*\n",
    "\n",
    "4. **Model Compression and Deployment:**  \n",
    "   *Is it feasible to distill sparse models into smaller, dense models while preserving performance improvements?*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff624e5",
   "metadata": {},
   "source": [
    "## 2.2 High-level Structure  \n",
    "\n",
    "A Switch Transformer layer is identical to a standard Transformer block **except** that the\n",
    "dense Feed-Forward Network (FFN) is replaced by a **Switch-FFN** (sparse Mixture-of-Experts).  \n",
    "Figure 2 from the paper illustrates the encoder block with two example tokens (${x_1,x_2}$).  \n",
    "Only the shaded *Switch-FFN* (light-blue) differs from a dense model.  :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure2_switch_arch.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure 2</b> – Switch-FFN inside the Transformer block (Fedus et al., 2022).</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "| Component | Role |\n",
    "| :-- | :-- |\n",
    "| **Self-Attention** | Context mixing (unchanged). |\n",
    "| **Router** | Lightweight linear layer that decides **which expert** handles each token. |\n",
    "| **N Experts** | Independent MLPs (usually GELU) with identical shapes but **distinct parameters**. |\n",
    "| **Load-Balancing Loss** | Keeps traffic roughly uniform across experts. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb742a",
   "metadata": {},
   "source": [
    "## 2.3 Routing & Gating Mechanism\n",
    "\n",
    "### **Overview**\n",
    "\n",
    "The critical innovation of **Switch Transformers** lies in the routing mechanism—specifically, how each token dynamically selects exactly one expert out of several available experts (MLPs). This approach dramatically reduces computational overhead while allowing massive scaling in model size.\n",
    "\n",
    "\n",
    "### **Router Inputs**\n",
    "\n",
    "Each Transformer layer typically consists of Self-Attention followed by Layer Normalization (LayerNorm). In a standard Transformer, the resulting hidden state would directly enter a dense Feed-Forward Network (FFN).\n",
    "\n",
    "In **Switch Transformers**, the router receives the **same hidden state**—specifically, the output embedding after the Self-Attention block and LayerNorm step: $\\mathbf{x} \\in \\mathbb{R}^{d_{\\text{model}}}$\n",
    "\n",
    "Here, $ d_{\\text{model}} $ denotes the embedding dimensionality.\n",
    "\n",
    "The router input is thus fully contextualized at the token level but does not depend directly on other tokens' routing decisions, which allows independent per-token routing computations.\n",
    "\n",
    "### **Routing Computation**\n",
    "\n",
    "The routing mechanism is computed as follows:\n",
    "\n",
    "**Step-by-Step:**\n",
    "\n",
    "1. **Router Projection (Compute logits)**\n",
    "\n",
    "The router projects the token representation into a space of dimension equal to the number of experts \\( N \\):\n",
    "\n",
    "$$\n",
    "h_i(\\mathbf{x}) = \\mathbf{w}_i^\\top\\mathbf{x} + b_i,\\quad \\forall i\\in\\{1,\\dots,N\\}\n",
    "$$\n",
    "\n",
    "Each expert has its own set of weights $ \\mathbf{w}_i \\in \\mathbb{R}^{d_{\\text{model}}} $ and biases $ b_i $.\n",
    "\n",
    "2. **Softmax Normalization**\n",
    "\n",
    "The logits are normalized via softmax to form a probability distribution over the experts:\n",
    "\n",
    "$$\n",
    "p_i(\\mathbf{x}) = \\frac{\\exp(h_i(\\mathbf{x}))}{\\sum_{j=1}^{N}\\exp(h_j(\\mathbf{x}))}\n",
    "$$\n",
    "\n",
    "3. **Jitter Addition (Exploration Noise)**\n",
    "\n",
    "A small uniform noise (\"jitter\") is added during training to encourage exploration of different routing decisions and prevent early convergence (collapse):\n",
    "\n",
    "$$\n",
    "\\tilde{p}_i(\\mathbf{x}) = p_i(\\mathbf{x}) + \\epsilon,\\quad \\epsilon\\sim\\mathcal{U}(0,10^{-2})\n",
    "$$\n",
    "\n",
    "4. **Top-1 Selection**\n",
    "\n",
    "The Switch Transformer chooses exactly **one expert** per token (top-1 selection):\n",
    "\n",
    "$$\n",
    "i^{\\star} = \\arg\\max_{i}\\tilde{p}_i(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "5. **Expert Processing**\n",
    "\n",
    "The selected expert (MLP) processes the token embedding:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = E_{i^{\\star}}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Each expert $E_i$ is a two-layer MLP with GELU activation, independent parameters, and identical structure across experts.\n",
    "\n",
    "6. **Gate-weighted Output**\n",
    "\n",
    "The output from the expert is multiplied by the original gate probability (without jitter) to keep the routing decision differentiable and allow gradient flow through the router:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = p_{i^{\\star}}(\\mathbf{x})\\,\\mathbf{e}\n",
    "$$\n",
    "\n",
    "This operation, known as a \"straight-through estimator,\" ensures smooth training despite discrete expert selection.\n",
    "\n",
    "\n",
    "### **Capacity and Hard Limit**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure3_capacity_dynamics.png\" width=\"650\"/>\n",
    "  <figcaption><b>Figure 3 — Token-routing dynamics under two capacity factors.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "**What the diagram shows**\n",
    "\n",
    "*Left panel (capacity 1.0)*  \n",
    "* 12 tokens must be routed across three experts (rows).  \n",
    "* Each expert’s capacity is exactly the ideal load \\(B/N = 4\\) tokens.  \n",
    "* Because many tokens happen to share the same favourite expert, that expert\n",
    "  overflows — the dotted red boxes show the **dropped** tokens that will bypass\n",
    "  this layer.\n",
    "\n",
    "*Right panel (capacity 1.5)*  \n",
    "* The capacity per expert is now \\(4 \\times 1.5 = 6\\) tokens, giving 50 % slack.  \n",
    "* All 12 tokens fit; no red overflow boxes, but a few **empty white slots** indicate\n",
    "  wasted compute/communication.\n",
    "\n",
    "**Take-away** A small slack margin (the paper standardises on 1.25) almost\n",
    "eliminates overflows yet keeps extra FLOPs and bandwidth modest.  Capacity is\n",
    "therefore the runtime *circuit-breaker* that guarantees fixed memory and latency\n",
    "even when the router’s token-to-expert distribution is skewed.\n",
    "\n",
    "**Why does capacity matter?**\n",
    "\n",
    "Routing decisions can lead to unbalanced token assignments, with some experts overloaded while others remain underutilized. Without mitigation, overloaded experts would cause out-of-memory errors, degraded performance, and unpredictable latency.\n",
    "\n",
    "Switch Transformer addresses this via a **hard capacity limit** on each expert:\n",
    "\n",
    "$$\n",
    "\\text{capacity per expert} = \\left\\lceil \\frac{B}{N} \\times \\text{capacity\\_factor} \\right\\rceil\n",
    "$$\n",
    "\n",
    "- $ B $: Total number of tokens in the micro-batch\n",
    "- $ N $: Number of experts\n",
    "- $\\text{capacity\\_factor}$: Typically set to $1.25$\n",
    "\n",
    "### **Load-Balancing Losses**\n",
    "\n",
    "Early attempts at Mixture-of-Experts models encountered a critical problem known as **expert collapse**, where a small number of experts dominated token assignments, starving others of training signal. Switch Transformers introduced two auxiliary losses to prevent collapse and encourage balanced expert usage:\n",
    "\n",
    "#### 1. **Auxiliary Load-Balancing Loss**\n",
    "\n",
    "$$\n",
    "L_{\\text{aux}} = \\alpha N \\sum_{i=1}^{N} f_i P_i,\\quad\n",
    "f_i = \\frac{\\text{tokens routed to expert } i}{\\text{batch size}},\\quad\n",
    "P_i = \\text{average gate probability for expert } i\n",
    "$$\n",
    "\n",
    "**Intuition:**  \n",
    "- Encourages even token distribution by penalizing high correlation between an expert’s frequency of selection and router confidence.\n",
    "- If an expert is frequently chosen and with high confidence, the loss increases, nudging the model to utilize other experts more evenly.\n",
    "\n",
    "#### 2. **Z-Loss (Logit Regularization)**\n",
    "\n",
    "$$\n",
    "L_z = \\beta \\sum_{\\text{tokens}}\\sum_{i}(h_i - \\text{stop\\_grad}(h_i))^2\n",
    "$$\n",
    "\n",
    "**Intuition:**  \n",
    "- Encourages router logits $h_i$ to remain small in magnitude, preventing overly confident selections (extremely large or small logits can destabilize softmax distributions).\n",
    "- Maintains stable gradient flow and well-conditioned router softmax.\n",
    "\n",
    "**Effectiveness:**\n",
    "- These combined losses improve expert utilization entropy, reduce gradient variance across experts, accelerate training convergence, and improve overall training stability.\n",
    "- Compared to earlier MoE methods, these light-weight auxiliary losses significantly improve model scalability and robustness with minimal computational overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a96b60",
   "metadata": {},
   "source": [
    "## 2.5 Experiments & Results\n",
    "### Baseline Models Used for Comparison  \n",
    "\n",
    "The authors selected **four families** of comparison models, each serving a distinct purpose.\n",
    "\n",
    "####   Dense T5 Series – \n",
    "**T5** stands for **“Text-to-Text Transfer Transformer.”**  \n",
    "Released by Google in late 2019, it introduced a simple yet powerful idea: *cast every NLP task—translation, summarisation, QA, sentiment, …—as feeding one piece of text in and predicting another piece of text out.*  \n",
    "This unification plus large-scale span-corruption pre-training on the **C4** web corpus produced a strong encoder–decoder baseline.\n",
    "\n",
    "**Why the authors picked T5:**\n",
    "\n",
    "1. **Like-for-like objective and codebase** – eliminates spurious gains from task formulation or optimiser tweaks.  \n",
    "2. **Widely reported benchmarks** – GLUE / SuperGLUE / SQuAD scores for T5 are standard yard-sticks, so improvements are easy to contextualise.  \n",
    "3. **Scales up smoothly** – letting the paper test whether sparse scaling beats a dense model simply made *bigger* (e.g., T5-Large or T5-XXL).\n",
    "\n",
    "Thus, throughout the experiments T5 provides a **clean, well-understood dense baseline** against which the efficiency and quality of the Switch (sparse) approach can be judged.\n",
    "\n",
    "\n",
    "| Model | Params | FLOPs / token | Why chosen |\n",
    "|-------|--------|--------------|-----------|\n",
    "| **T5-Base** | 223 M | 1 × (reference) | Same size class as 2-expert Switch; establishes a dense baseline that already fits on a single TPU/GPU. |\n",
    "| **T5-Large** | 739 M | 3.5 × Switch-Base | Represents a “scale-up dense” strategy within the same architecture and codebase. |\n",
    "| **T5-XXL** | 11 B | 6.3 T per seq | State-of-the-art dense model at publication time; tests whether sparse can outpace *very* large dense models under the same cluster budget. |\n",
    "\n",
    "*Rationale* – All T5 variants share the *exact* training objective, tokenizer, and optimizer code. That isolates the effect of **conditional vs dense compute**.\n",
    "\n",
    "####   MoE Transformer (Top-2 Routing) – \n",
    "\n",
    "| Variant | Experts | Routing | Why chosen |\n",
    "|---------|---------|---------|-----------|\n",
    "| **MoE-Transformer (Shazeer et al.)** | 128 | top-2 | Prevailing MoE design before Switch; higher FLOPs because two experts fire per token. |\n",
    "\n",
    "*Rationale* – Validates whether **single-expert** routing is genuinely more efficient/stable than the established top-k approach.\n",
    "\n",
    "#### Why these baselines are fair  \n",
    "\n",
    "* **Same tokenizer and data** → eliminates corpus effects.  \n",
    "* **Same optimizer hyper-params** (where feasible) → isolates architectural difference.  \n",
    "* **FLOP-matched pairs** (Switch-Base vs T5-Base, Switch-Large vs T5-Large) → asks:  \n",
    "  > *“Given the **same compute budget**, which architecture learns faster / better?”*  \n",
    "* **Higher-FLOP dense models** (T5-Large, -XXL) → test the critique  \n",
    "  > *“Just spend more FLOPs on dense; why bother with sparsity?”*  \n",
    "* **Legacy MoE top-2** → ensures the improvement isn’t merely “MoE vs dense” but due to the **Switch simplification**.\n",
    "\n",
    "Using this spectrum of baselines, the paper demonstrates that Switch Transformers outperform:\n",
    "\n",
    "1. **Compute-matched dense** models (fair efficiency test),  \n",
    "2. **Heavier dense** models (efficiency-vs-quality frontier), and  \n",
    "3. **Previous sparse** architectures (methodological advance).\n",
    "\n",
    "This comprehensive baseline suite strengthens the claim that **conditional compute via single-expert routing is a superior scaling path**.\n",
    "\n",
    "###  Scaling Properties \n",
    "The paper’s **Scaling Properties** section asks:  \n",
    "> *“If we keep FLOPs / token roughly constant, how far can we improve quality by adding more experts (i.e., more parameters)?”*  \n",
    "\n",
    "To answer, the authors run three tightly-controlled experiments.\n",
    "\n",
    "####  Step-Basis Scaling\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_4_step_basis_scaling.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure: Scaling Switch Transformer (perplexity vs. training steps and wall-clock).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Pre-train **Switch-Base** models with 2 → 256 experts (223 M → 14.7 B parameters) **for a fixed 100 k steps**. FLOPs/token stay constant because each token still activates one expert. |\n",
    "| **Purpose** | Is parameter-only scaling (via experts) a free win when compute is fixed? |\n",
    "| **Main results** | Perplexity **drops monotonically** as experts double. The 64-expert model matches T5-Base quality **7.5 × sooner** in steps. |\n",
    "| **Interpretation** | Extra capacity (parameters) is effectively used even though compute is unchanged. Sparse routing is therefore a *new scaling axis* orthogonal to FLOPs. |\n",
    "\n",
    "####  Time-Basis Scaling\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_5_time_basis_scaling.png\" width=\"300\"/>\n",
    "  <figcaption><b>Figure: Scaling Switch Transformer (perplexity vs. training time and wall-clock).</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Measure **wall-clock minutes** to reach target perplexities on identical TPU pods. Same model family as above. |\n",
    "| **Purpose** | Extra experts add routing overhead (softmax, All-to-All). Do they erase the step advantage? |\n",
    "| **Main results** | Sparse models *still* win: 64-expert Switch reaches T5-Base quality in **≈ 140 min vs 350 min** (≈ 2.5× faster). |\n",
    "| **Conclusion** | Routing + communication overhead is small relative to the gains from parameter scaling. Sparse models give **real-time savings**, not just step savings. |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "####  Sparse vs. “Just Make the Dense Model Bigger”\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/figure_6_parameters_basis_scaling.png\" width=\"600\"/>\n",
    "  <figcaption><b>Figure: Sample Efficiency Switch Transformer VS T5 variants.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Compare **Switch-Base (64 e)** against **T5-Large** which spends **3.5× more FLOPs/token** than Switch-Base. |\n",
    "| **Purpose** | Critics could argue “dense scaling already works—just spend more FLOPs.” |\n",
    "| **Main results** | Despite T5-Large’s heavier compute, Switch-Base is **2.5× faster** to the same perplexity and *still* ends lower. |\n",
    "| **Conclusion** | Conditional computation **dominates** naive dense scaling in the speed/quality trade-off. You can’t buy the same improvement just by burning more FLOPs per token. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759c905",
   "metadata": {},
   "source": [
    "###  Down-Stream Experiments\n",
    "\n",
    "The authors performed five focused studies to verify that the pre-training gains of Switch Transformers **transfer** to real tasks and to understand how best to fine-tune, regularise, and deploy very large sparse models.\n",
    "\n",
    "\n",
    "####  Fine-Tuning Benchmark Suite\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/table_5_fine_tuning_results.png\" width=\"400\"/>\n",
    "  <figcaption><b>Table: Fine-tuning results. T5 baselines VS Switch models across\n",
    "  a diverse set of natural language test.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "- **GLUE —** A bundle of nine sentence‐level and sentence-pair evaluations (sentiment, paraphrase, natural-language inference, etc.) that together gauge broad language understanding in English.\n",
    "\n",
    "- **SuperGLUE —** A harder successor to GLUE featuring multi-sentence reasoning tasks such as BoolQ, ReCoRD, and WSC; designed to test deeper compositional reasoning and commonsense.\n",
    "\n",
    "- **SQuAD v1.1 —** Reading-comprehension question answering on Wikipedia passages; checks the model’s ability to locate and extract exact answer spans from context.\n",
    "\n",
    "- **XSum —** Single-sentence abstractive news summarisation; evaluates whether the system can condense an article into one concise, fluent sentence while preserving key facts.\n",
    "\n",
    "- **Winogrande —** Commonsense pronoun-resolution puzzles; measures the model’s grasp of implicit world knowledge needed to resolve ambiguous references.\n",
    "\n",
    "- **TriviaQA (closed-book) —** Open-domain factoid QA answered without external documents; probes how much factual knowledge is stored internally in the model’s parameters.\n",
    "\n",
    "- **ANLI —** Adversarial natural-language inference collected via model-in-the-loop annotation; assesses robustness to deliberately tricky NLI examples.\n",
    "\n",
    "- **ARC (Easy & Challenge) —** Multiple-choice grade-school science-exam questions; tests logical reasoning over short factual statements rather than surface pattern matching.\n",
    "\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Setup** | Fine-tuned FLOP-matched pairs: **Switch-Base (7 B) vs T5-Base (0.2 B)** and **Switch-Large (26 B) vs T5-Large (0.7 B)** on GLUE, SuperGLUE, SQuAD, XSum, Winogrande, TriviaQA, ANLI, ARC. Dropout: 0.1 non-expert, 0.4 expert; 100 k training steps. |\n",
    "| **Purpose** | Confirm that sparse pre-training advantages appear on diverse NLU, QA, and summarisation tasks. |\n",
    "| **Main results** | • **+4.4 pp SuperGLUE** (Base) and +2 pp (Large).<br>• Closed-book TriviaQA +6 pp.<br>• Gains on Winogrande, XSum; mixed on ARC. |\n",
    "| **Conclusion** | Pre-training gains **transfer broadly**; sparse capacity especially helps knowledge-heavy tasks. |\n",
    "\n",
    "####  Distillation for Deployment\n",
    "\n",
    "##### How knowledge distillation works in general  \n",
    "\n",
    "1. **Run the teacher** – pass each input through the (large) teacher model  \n",
    "   * obtain either the **logits** $z^{(T)}$ or the softened probabilities  \n",
    "     $\\sigma(z^{(T)}/T)$ at temperature $T>1$.\n",
    "\n",
    "2. **Run the student** – pass the *same* input through the small model  \n",
    "   to produce logits $z^{(S)}$.\n",
    "\n",
    "3. **Blend two losses**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\n",
    "  = \\lambda \\; \\underbrace{\\mathrm{KL}\\!\\bigl[\\sigma(z^{(T)}/T)\\;\\|\\;\\sigma(z^{(S)}/T)\\bigr]}_{\\text{soft-target loss}}\n",
    "  + (1-\\lambda)\\; \\underbrace{\\mathrm{CE}\\!\\bigl[y,\\;\\sigma(z^{(S)})\\bigr]}_{\\text{hard-target loss}}\n",
    "$$\n",
    "\n",
    "* $y$ – ground-truth labels  \n",
    "* $\\sigma$ – softmax  \n",
    "* $\\lambda$ – weight that trades off “mimic the teacher” vs. “fit the labels”  \n",
    "* Back-propagate **only through the student**; the teacher is frozen.\n",
    "\n",
    "4. **Optimise the student** until validation perplexity or task metric plateaus.  \n",
    "   The student thus learns a compressed approximation of the teacher’s behaviour\n",
    "   while still respecting the original task labels.\n",
    "\n",
    "\n",
    "##### Switch-Transformer distillation  \n",
    "\n",
    "* **Teacher models** – sparse Switch-Base variants  \n",
    "  * 3.8 B , 7.4 B , 14.7 B parameters (64 or 128 experts)  \n",
    "  * already pre-trained on C4; one run fine-tuned on SuperGLUE.\n",
    "\n",
    "* **Student model** – dense T5-Base, 223 M parameters.\n",
    "\n",
    "* **Weight initialisation** – copy all **non-expert weights** (embeddings, attention,\n",
    "  residual projections) from teacher to student; randomly init the rest.\n",
    "\n",
    "* **Soft / hard mix** –  \n",
    "  $\\lambda = 0.25$ for the soft-target KL term, $1-\\lambda = 0.75$ for the standard cross-entropy.\n",
    "\n",
    "* **Temperature** – $T = 2.0$ to soften the teacher’s probability distribution.\n",
    "\n",
    "* **Training data & length** –  \n",
    "  * C4 span-corruption for language modelling distillation (150 k steps).  \n",
    "  * SuperGLUE labelled set for task-specific distillation (same step budget).\n",
    "\n",
    "* **Optimiser & schedule** – identical Adafactor settings used in pre-training; no extra tricks.\n",
    "\n",
    "* **Outcome** – student keeps **≈30 %** of the teacher’s quality gain while shrinking\n",
    "  model size by **95–99 %**, demonstrating a deployable path for Switch-Transformer\n",
    "  knowledge.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"switch_plots/table_6_distillition.png\" width=\"400\"/>\n",
    "  <figcaption><b>Table: Fine-tuning results. T5 baselines VS Switch models across\n",
    "  a diverse set of natural language test.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **What was done** | Distilled sparse **Switch-Base 3.8 B / 7.4 B / 14.7 B** teachers into a 223 M dense T5-Base student. Tricks: initialise student with teacher’s non-expert weights + 0.25 × soft-loss + 0.75 × hard-loss. |\n",
    "| **Why** | Massive trillion-parameter models are hard to deploy; distillation offers a lighter alternative. |\n",
    "| **Main results** | • **≈ 30 % of teacher gain retained** at 95–99 % compression.<br>• Fine-tuned SuperGLUE distillation keeps 30 % gain on a 97 % compressed model. |\n",
    "| **Interpretation** | Distillation provides a **practical path** from huge sparse teachers to deployable dense students while preserving a meaningful slice of quality improvement. |\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c02a4",
   "metadata": {},
   "source": [
    "## 2.6 Contributions\n",
    "\n",
    "The paper's main contributions are:\n",
    "\n",
    "- Introducing **single-expert (top-1) routing**, drastically simplifying routing and reducing computational overhead.\n",
    "- Presenting clear evidence of performance improvements across diverse NLP benchmarks, validating the model’s practical utility.\n",
    "- Proposing effective model compression through knowledge distillation, enabling deployment of large model capabilities into significantly smaller models.\n",
    "\n",
    "Together, these findings validate the viability of conditional computation at unprecedented scale.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57167407-c9a5-429f-87fb-1de5c50a6951",
   "metadata": {
    "id": "57167407-c9a5-429f-87fb-1de5c50a6951"
   },
   "source": [
    "## 3 PEER (parameter efficient expert retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b9f11-e31a-41a5-b8e6-813bf37d7c68",
   "metadata": {
    "id": "f80b9f11-e31a-41a5-b8e6-813bf37d7c68"
   },
   "source": [
    "### 3.0 Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a768a-a2b2-4fec-a5ea-af30c20dbfd9",
   "metadata": {
    "id": "575a768a-a2b2-4fec-a5ea-af30c20dbfd9"
   },
   "source": [
    "Just to be alligned, we'll explain here the following terms:\n",
    "\n",
    "* **expert** - A specialized model or sub-model intended to handle a specific subset of data or tasks. It can be assumed as a small neural network (Usually some few neurons) which is a sub part of a layer.\n",
    "* **product key** - A mechanism to enable efficient and effective matching between input data and the best-suited experts to process that data in a vast and diverse pool of potential experts. Every product key is a vector which represents the features or aspects of inputs that the expert is particularly suited to handle.\n",
    "* **feed forward layer** - A layer which the information is passed in one direction, from input to output, without any loops or feedback within the layer itself, and with no backward propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae322a-6a7f-4b44-b5e4-57bc15d704a5",
   "metadata": {
    "id": "ceae322a-6a7f-4b44-b5e4-57bc15d704a5"
   },
   "source": [
    "### 3.1 Introduction and motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5675abf-bf46-4439-9efb-4f57ee397fd3",
   "metadata": {
    "id": "d5675abf-bf46-4439-9efb-4f57ee397fd3"
   },
   "source": [
    "- Based on the following paper: https://arxiv.org/pdf/2407.04153v1.\n",
    "- The main idea here is the innovation of the sparse mixture-of-experts (MoE) model architectures using PEER (parameter efficient expert retrieval) layer.\n",
    "- The PEER layer includes a vast number of tiny expert (over a million) and solves one of the main issues of the traditional models:\n",
    "  - In traditional MoE models, the feedforward layers (FFW) have a linear increase in computational costs and activation memory as the hidden layer width grows.\n",
    "  - However, in PEER, by enabling efficient utilization of a massive number of experts, it can support a further scaling of transformer models while maintaining almost the same computational efficiency.\n",
    "  - It uses the product key technique for sparse retrieval.\n",
    "\n",
    "The main contributions of this work are:\n",
    "- ***Exploration of Extreme MoE Setting:*** Unlike previous MoE research, this work investigates the under-explored case of numerous tiny experts.\n",
    "- ***Learned Index Structure for Routing:*** Demonstrating for the first time that a learned index structure\n",
    "(Kraska et al., 2018) can efficiently route to over a million experts.\n",
    "- ***New Layer Design:*** Combining product key routing with single-neuron experts, we introduce the\n",
    "PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its major efficiency compared to MoE architectures in previous researches.\n",
    "- ***Comprehensive Ablation Studies:*** Investigating the impact of different design choices of PEER\n",
    "such as number of experts, active parameters, number of heads and query batch normalization on\n",
    "language modeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de9cd4-23cb-4084-9c74-a7fbe21d5623",
   "metadata": {
    "id": "14de9cd4-23cb-4084-9c74-a7fbe21d5623"
   },
   "source": [
    "### 3.2 Architecture and mathematical equalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6aef6-c0ce-4246-8821-0ab1ca0db463",
   "metadata": {
    "id": "2bf6aef6-c0ce-4246-8821-0ab1ca0db463"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/Peer_Layer.png?raw=1\" alt=\"Peer_Layer\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19cc5dd-7fe5-4a11-80c4-807d84608e65",
   "metadata": {
    "id": "f19cc5dd-7fe5-4a11-80c4-807d84608e65"
   },
   "source": [
    "The following is an illustration of the PEER layer:\n",
    "* A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers\n",
    "* Given the state vector x from the previous layer, a query\n",
    "network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the\n",
    "router scores and to retrieve the top $k$ experts $e_1$, ..., $e_k$\n",
    "* After the retrieved experts make their predictions\n",
    "ei(x), their outputs are linearly combined using the softmax-normalized router scores as weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132de35-4f8d-40b5-b873-7f9a41bffd7d",
   "metadata": {
    "id": "c132de35-4f8d-40b5-b873-7f9a41bffd7d"
   },
   "source": [
    "Formally, a PEER layer is a function $f:{R^n}\\rightarrow{R^m}$ that consists of three parts:\n",
    "* A pool of $N$ experts $E:=\\{{e_i}\\}_{i=1}^{N}$ where each expert $e_i:{R^n}\\rightarrow{R^m}$ shares the same signature as $f$\n",
    "* A matching set of $N$ produect keys: $K:=\\{{k_i}\\}_{i=1}^{N} \\subset R^d$ ($d$ is the dimension of the vectors)\n",
    "* A query network $q:{R^n}\\rightarrow{R^d}$ that maps the input vector $x\\in{R^n}$ to a query vector $q(x)$\n",
    "\n",
    "The layer output is done by the following three steps:\n",
    "1. Retrieving the top $k$ experts: The experts who their corresponding product keys have the highest inner products with the query $q(x)$: $I=T_k(\\{q(x)^Tk_i\\})_{i=1}^{N}$ while $T_k$ is the top $k$ operator\n",
    "2. Actication: Applying nonlinear activations (such as softmax or sigmoid) to the query-key inner products of these\n",
    "top $k$ experts to obtain the router scores: $g_i(x) = s(q(x)^Tk_i)$\n",
    "3. Output: Computing the output by linearly combining the expert outputs weighted by the router scores: $f(x) = \\sum_{{i}\\in{I}}g_i(x)e_i(x)$.\n",
    "\n",
    "How are the most suitable $k$ experts are actually chosen?\n",
    "1. Query vectors Generation: A set of query vectors is generated from the input vectors. These query vectors are created specifically for the task of selecting the appropriate experts. Each product in the \"Product of Experts\" approach has its own query vectors.\n",
    "2. Scoring of query vectors: Each query vector is then scored based on its compatibility with the product keys associated with the available experts. This scoring is typically done using a similarity measure, such as a dot product.\n",
    "3. Top-k Selection: The scores determine the relevance or importance of each expert for the given input vector x. The k highest scoring experts (i.e., those with the keys having the highest similarity scores) are selected as the active experts for that particular input\n",
    "\n",
    "Product key retrieval - Efficiency:\n",
    "* Since $N$ can be a huge number $(N \\geq 10^6)$, we don't want to compute it by the naive way\n",
    "* Instead of using $N$ independent $d$-dimensional vectors as our product keys $k_i$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys $C, C' \\subset R^{\\frac{d}{2}}$, $|C|=|C'|=\\sqrt{N}$\n",
    "* So in total $K = {\\{[_{c'}^{c}]|c\\in{C}, c'\\in{C'}\\}}$\n",
    "* Hence we choose $N$ to be a perfect square and $d$ to be an even number\n",
    "* Instead of comparing $q(x)$ to all $N$ keys in $K$ and selecting the top $k$ matches, we split the query vector $q(x)$ into two subqueries $q1$ and $q2$ (Each one of them is d/2 dimensional) and apply the top $k$ operations to the inner products between the sub-queries and sub-keys\n",
    "respectively\n",
    "* This results in a set of $k^2$ candidate keys, and it is mathematically guaranteed that the $k$ most similar keys to $q(x)$ from $K$ are in this candidate set so we can simply apply the top-k operator again to these $k^2$ inner products to get the top $k$ matching keys from the original set of product keys $K$\n",
    "* Naive way runtime complexity: $O(Nd)$\n",
    "* Efficient way runtime complexity: $O((\\sqrt{N}+k^2)d)$\n",
    "\n",
    "\n",
    "Parameter Efficient Experts and Multi-Head Retrieval:\n",
    "* In other MoE architectures, the hidden layer of each expert is set to the same size as other FFW layers (A few and large experts)\n",
    "* In PEER, every expert is a one hidden layer with a single neuron $e_i(x) := \\sigma(u_i^Tx)v_i$ while $\\sigma$ is a non-linear activation function\n",
    "* Instead of making the size of individual experts different, we are using a multihead retrieval of $h$ independent query networks instead of one, each computes its own query and retrieves a separate set of $k$ experts: $f(x) := \\sum_{i=1}^{h}f^i(x) = \\sum_{i=1}^{h}\\sum_{j \\in I^i} g^j(x)e^j(x)$\n",
    "\n",
    "How can an expert be efficient if it has just a single Neuron ?\n",
    "1. During training, the neuron's weights are adjusted to respond preferentially to specific patterns in the data. For instance, a neuron could be tuned to react strongly to certain spatial features in an image or a text.\n",
    "2. Usually, a single neuron can effectively perform binary classification based on whether the input meets a certain condition (e.g., whether the aggregated weighted sum reaches a particular threshold). Usually a single neuron has a specialized feature.\n",
    "3. Its output would be combined with that of other experts (which could be either neurons or more complex networks), each handling different features or aspects of the data. This combination would typically be a weighted sum or another associative operation, guiding the final decision or output of the PEER layer.\n",
    "\n",
    "Hyperparameters and loss function:\n",
    "* There are three main hyperparameters to a standard MoE layer:\n",
    "  1. $P$ - Total number of parameters\n",
    "  2. $P_{active}$ - Total number of active parameters per token\n",
    "  3. $P_{expert}$ - The size of a single expert\n",
    "* Two more important terms:\n",
    "  1. $D$ - Number of training tokens\n",
    "  2. $G := \\frac{P_{active}}{P_{experts}}$ - Number of active experts\n",
    "* Now the loss function is $L(P, D, G) = c + (\\frac{g}{G^\\lambda} + a)\\frac{1}{P^\\alpha} + \\frac{b}{D^\\beta}$ ($a, b, c, g, \\alpha, \\beta, \\lambda$ are constants)\n",
    "* We would like to scale the total number of parameters, of experts and of the tokens, but NOT the number of active parameters, since the number of the active parameters affects the computational and runtime costs\n",
    "* Since we want to increase the number of experts, we need to decrease the size of each expert if we do not increase the number of active parameters. Hence we need a large number of small experts\n",
    "* In PEER, we set $P_{experts}$ to 1 and $P_{active}$ is the number of retrieval heads multiplied by the number of experts retrieved per head which is $hk$. Hence, in PEER layer the number of active experts is $G = hk$\n",
    "\n",
    "Below is a Pseudo Code sample of a PEER Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9e754-31da-4fc7-b5b6-6f7c965c6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import math\n",
    "from einops import einsum\n",
    "from tqdm import tqdm \n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class PEER(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        heads=8,\n",
    "        num_experts=1_000_000,\n",
    "        num_experts_per_head=16,\n",
    "        activation=nn.GELU,\n",
    "        dim_key=None,\n",
    "        product_key_topk=None,\n",
    "        separate_embed_per_head=False,\n",
    "        pre_rmsnorm=False,\n",
    "        dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Here the norm is the torch identity of each torch\n",
    "        self.norm = RMSNorm(dim) if pre_rmsnorm else nn.Identity()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.separate_embed_per_head = separate_embed_per_head\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # There are some sets of experts as the number of heads\n",
    "        num_expert_sets = heads if separate_embed_per_head else 1\n",
    "\n",
    "        # Embedding layers storing the down /up projection weights of all experts\n",
    "        self.weight_down_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "        self.weight_up_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "\n",
    "        # The activation method is GELU (Gaussian Error Linear Unit)\n",
    "        self.activation = activation()\n",
    "\n",
    "        # As told, the number of experts must be a perfect square\n",
    "        assert (num_experts ** 0.5).is_integer(), '`num_experts` needs to be a square'\n",
    "\n",
    "        # As told, the number of dimension must be even number\n",
    "        assert (dim % 2) == 0, 'feature dimension should be divisible by 2'\n",
    "\n",
    "        # The dimension of each one of the product keys is d/2\n",
    "        dim_key = dim_key if exists(dim_key) else dim // 2\n",
    "\n",
    "        # The number of keys is the the root square of the number of experts\n",
    "        self.num_keys = int(num_experts ** 0.5)\n",
    "\n",
    "        # A linear layer of the query vectors\n",
    "        self.to_queries = nn.Sequential(\n",
    "            nn.Linear(dim, dim_key * heads * 2, bias=False),\n",
    "            Rearrange('b n (p h d) -> p b n h d', p=2, h=heads)\n",
    "        )\n",
    "\n",
    "        # The product key topK is the number of experts per head\n",
    "        self.product_key_topk = product_key_topk if exists(product_key_topk) else num_experts_per_head\n",
    "        self.num_experts_per_head = num_experts_per_head\n",
    "\n",
    "        # There are a total of h * root square of N keys and each one of them has a dimension of d/2\n",
    "        self.keys = nn.Parameter(torch.randn(heads, self.num_keys, 2, dim_key))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In our case, keep the input vectors x as is\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Build query vectors q(x) from x\n",
    "        queries = self.to_queries(x)\n",
    "\n",
    "        ###\n",
    "        # Retrieve the weights of the top matching experts using product keys\n",
    "        # indices and scores have the shape ’bnhk ’, where h is the number of heads\n",
    "        ###\n",
    "        # Einsum is a matrix-multicipation describing the dimension of each matrix X, Y -> Z <=> X*Y = Z\n",
    "        sim = einsum(queries, self.keys, 'p b n h d, h k p d -> p b n h k')\n",
    "        (scores_x, scores_y), (indices_x, indices_y) = [s.topk(self.product_key_topk, dim=-1) for s in sim]\n",
    "        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n",
    "        all_indices = indices_x.unsqueeze(-1) * self.num_keys + indices_y.unsqueeze(-2)\n",
    "        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n",
    "        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n",
    "        scores, pk_indices = all_scores.topk(self.num_experts_per_head, dim=-1)\n",
    "        indices = all_indices.gather(-1, pk_indices)\n",
    "\n",
    "        # Is false in our use case\n",
    "        if self.separate_embed_per_head:\n",
    "            head_expert_offsets = torch.arange(self.heads, device=x.device) * self.num_experts\n",
    "            indices = indices + head_expert_offsets.view(1, 1, -1, 1)\n",
    "\n",
    "        # Setup the down / up projection weights of all experts based on the indices\n",
    "        weights_down = self.weight_down_embed(pk_indices)\n",
    "        weights_up = self.weight_up_embed(pk_indices)\n",
    "\n",
    "        ####\n",
    "        # Compute weighted average of expert outputs\n",
    "        ####\n",
    "\n",
    "        # Matrix multicipation of the expert outputs and the bottom projected weights \n",
    "        x = einsum(x, weights_down, 'b n d, b n h k d -> b n h k')\n",
    "\n",
    "        # GELU Activation\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # Zero some of the outputs with a probability of \"dropout\" which is 0 in our case (Do not reset outputs)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Softmax compution\n",
    "        x = x * F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Matrix multicipation of the expert outputs and the upper projected weights \n",
    "        x = einsum(x, weights_up, 'b n h k, b n h k d -> b n d')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012373d-bae6-4434-a232-692cf8c59c60",
   "metadata": {
    "id": "5012373d-bae6-4434-a232-692cf8c59c60"
   },
   "source": [
    "### 3.3 Results of major experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5015af-e108-4a66-8774-f5ae9a99059f",
   "metadata": {
    "id": "cf5015af-e108-4a66-8774-f5ae9a99059f"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/Peer_Experiment.png?raw=1\" alt=\"PEER_Experiment\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421858d6-0557-42b3-b9d5-8e35cdf6a3a2",
   "metadata": {
    "id": "421858d6-0557-42b3-b9d5-8e35cdf6a3a2"
   },
   "source": [
    "The following is a graph of the train loss per batch of their training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621baeb-678b-4f57-921c-d7ccc874985a",
   "metadata": {
    "id": "9621baeb-678b-4f57-921c-d7ccc874985a"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/6e18_6e19_FLOPs.png?raw=1\" alt=\"6e18_6e19_FLOPs\" width=\"700\" height=\"500\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/Varying_N_hk.png?raw=1\" alt=\"Varying_N_hk\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab50e99-1c66-4491-84d1-2a400f6cd1ac",
   "metadata": {
    "id": "8ab50e99-1c66-4491-84d1-2a400f6cd1ac"
   },
   "source": [
    "* The following graphs in the upper side shows the perplexity per number of parameters for 2e19 (more efficient computation) and 2e18 (less efficient computation) FLOP's, comparing different MoE models:\n",
    "  * Dataset is $C4$\n",
    "  * Total number of experts $N$ is $1024^2$\n",
    "  * The number of active experts is $128$ for Dense, PEER and MOE and $256$ memories for PKM\n",
    "  * It can be shown that PEER gives the lower perplexity and with the largest number of parameters\n",
    "* The graph in the left-bottom side shows the perplexity per number of parameters of PEER models, comparing different number of total experts $N$\n",
    "  * The size of the active experts $hk$ is always $128$\n",
    "  * Dataset is $C4$\n",
    "* The graph in the right-bottom side shows the perplexity per number of $h$ independent query networks (Active experts)\n",
    "  * Total number of experts $N$ is always $1024^2$\n",
    "  * Dataset is $C4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4e99a-3ae3-46d6-a9c6-26c578c83c20",
   "metadata": {
    "id": "aaa4e99a-3ae3-46d6-a9c6-26c578c83c20"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/PEER_Experiment_Table.png?raw=1\" alt=\"PEER_Experiment_Table\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2321f72-dc8d-4f60-8ccd-ebe239dec7bd",
   "metadata": {
    "id": "d2321f72-dc8d-4f60-8ccd-ebe239dec7bd"
   },
   "source": [
    "The following table compares the perplexity per model (rows) and per dataset (columns):\n",
    "  * Total number of experts $N$ is $1024^2$\n",
    "  * The number of active experts is $128$ for Dense, PEER and MOE and $256$ memories for PKM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce778b-b88f-481a-b266-2a322a9441c5",
   "metadata": {
    "id": "f2ce778b-b88f-481a-b266-2a322a9441c5"
   },
   "source": [
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/BatchNormTable.png?raw=1\" alt=\"BatchNormTable\" width=\"600\" height=\"600\">\n",
    "<img src=\"https://github.com/stavco9/moe-llm-presentation/blob/main/BatchNormGraph.png?raw=1\" alt=\"BatchNormGraph\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf2edd-b70a-433f-b810-d85a38fcf496",
   "metadata": {
    "id": "46cf2edd-b70a-433f-b810-d85a38fcf496"
   },
   "source": [
    "The following table and graph shows the perplexity per number of experts $N$ (In the graph the number of experts $N$ is $1M$) and using two scenarios:\n",
    "* With no query batch normalization\n",
    "* With query batch normalization\n",
    "\n",
    "These are the following details of experiments:\n",
    "* Dataset is $C4$\n",
    "* Model is PEER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b5ff6-5f7c-42fa-ace0-fc9eea195c76",
   "metadata": {
    "id": "ee6b5ff6-5f7c-42fa-ace0-fc9eea195c76"
   },
   "source": [
    "### 3.4 Hands-on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a63e7d-904a-496b-a20f-0a7d0171ece7",
   "metadata": {
    "id": "c3a63e7d-904a-496b-a20f-0a7d0171ece7"
   },
   "outputs": [],
   "source": [
    "!pip install datasets --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JqhTlTihNRI8",
   "metadata": {
    "id": "JqhTlTihNRI8"
   },
   "outputs": [],
   "source": [
    "%rm -rf moe-llm-presentation\n",
    "!git clone https://github.com/stavco9/moe-llm-presentation\n",
    "%cd moe-llm-presentation/peer_main\n",
    "%ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c681d514-7d3a-4d56-9edf-3b110e743bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lEGeog9d9l66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEGeog9d9l66",
    "outputId": "7f2f7939-35dc-4a8f-f5f1-26dffbd0f97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': '50257', 'dim': '64', 'num_layers': '4', 'num_heads': '4', 'num_experts': '16384', 'top_k': '8', 'batch_size': '6', 'num_epochs': '6', 'learning_rate': '0.0001', 'dataset': 'Salesforce/wikitext'}\n",
      "Loading pretrained GPT2 tokenizer transformer\n",
      "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 212kB/s]\n",
      "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.59MB/s]\n",
      "merges.txt: 100% 456k/456k [00:00<00:00, 1.05MB/s]\n",
      "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 1.99MB/s]\n",
      "config.json: 100% 665/665 [00:00<00:00, 6.02MB/s]\n",
      "Finished loading pretrained GPT2 tokenizer transformer\n",
      "Initalizing PEER model\n",
      "Finished initalizing PEER model\n",
      "Loading datasets\n",
      "README.md: 100% 10.5k/10.5k [00:00<00:00, 43.2MB/s]\n",
      "test-00000-of-00001.parquet: 100% 733k/733k [00:00<00:00, 75.7MB/s]\n",
      "train-00000-of-00002.parquet: 100% 157M/157M [00:00<00:00, 350MB/s]\n",
      "train-00001-of-00002.parquet: 100% 157M/157M [00:00<00:00, 506MB/s]\n",
      "validation-00000-of-00001.parquet: 100% 657k/657k [00:00<00:00, 409MB/s]\n",
      "Generating test split: 100% 4358/4358 [00:00<00:00, 461422.14 examples/s]\n",
      "Generating train split: 100% 1801350/1801350 [00:01<00:00, 908135.61 examples/s]\n",
      "Generating validation split: 100% 3760/3760 [00:00<00:00, 632899.23 examples/s]\n",
      "Filter: 100% 1801350/1801350 [00:04<00:00, 402742.27 examples/s]\n",
      "Filter: 100% 3760/3760 [00:00<00:00, 315582.08 examples/s]\n",
      "Finished loading datasets\n",
      "Number of parameters: 23703808\n",
      "Epoch Training 1/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.88it/s]\n",
      "Epoch Validation 1/6\n",
      "100% 411/411 [00:17<00:00, 22.95it/s]\n",
      "Epoch 1/6, Train Loss: 1.8483, Val Loss: 1.4844, Val Perplexity: 4.4124\n",
      "Epoch Training 2/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.88it/s]\n",
      "Epoch Validation 2/6\n",
      "100% 411/411 [00:17<00:00, 23.00it/s]\n",
      "Epoch 2/6, Train Loss: 1.3455, Val Loss: 1.8030, Val Perplexity: 6.0680\n",
      "Epoch Training 3/6\n",
      "100% 50000/50000 [1:24:22<00:00,  9.88it/s]\n",
      "Epoch Validation 3/6\n",
      "100% 411/411 [00:17<00:00, 22.99it/s]\n",
      "Epoch 3/6, Train Loss: 1.2391, Val Loss: 1.8606, Val Perplexity: 6.4276\n",
      "Epoch Training 4/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.88it/s]\n",
      "Epoch Validation 4/6\n",
      "100% 411/411 [00:17<00:00, 23.03it/s]\n",
      "Epoch 4/6, Train Loss: 1.1785, Val Loss: 1.8852, Val Perplexity: 6.5876\n",
      "Epoch Training 5/6\n",
      "100% 50000/50000 [1:24:23<00:00,  9.87it/s]\n",
      "Epoch Validation 5/6\n",
      "100% 411/411 [00:17<00:00, 22.88it/s]\n",
      "Epoch 5/6, Train Loss: 1.1468, Val Loss: 1.8965, Val Perplexity: 6.6626\n",
      "Epoch Training 6/6\n",
      "100% 50000/50000 [1:24:22<00:00,  9.88it/s]\n",
      "Epoch Validation 6/6\n",
      "100% 411/411 [00:17<00:00, 23.05it/s]\n",
      "Epoch 6/6, Train Loss: 1.1283, Val Loss: 1.8989, Val Perplexity: 6.6784\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8192\"\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 50257  # GPT-2 tokenizer vocab size\n",
    "dim = 64 # The dimension of each query vector d\n",
    "num_layers = 4 # The number of peer layers\n",
    "num_heads = 4 # The number of heads h\n",
    "num_experts = 128 * 128 # The total number of experts N\n",
    "top_k = 8 # The top k to use for each head\n",
    "batch_size = 6 # The batch size of each input\n",
    "num_epochs = 6 # Total number of epochs\n",
    "learning_rate = 1e-4\n",
    "dataset = 'Salesforce/wikitext'\n",
    "\n",
    "!torchrun --nproc_per_node=1 --nnodes=1 main.py --vocab-size={vocab_size} --dim={dim} \\\n",
    "  --num-layers={num_layers} --num-heads={num_heads} --num-experts={num_experts} --top-k={top_k} --batch-size={batch_size} \\\n",
    "  --num-epochs={num_epochs} --learning-rate={learning_rate} --dataset={dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "827308a8-6d47-40a2-b514-8330dc28c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = np.array([1.8483, 1.3455, 1.2391, 1.1785, 1.1468, 1.1283])\n",
    "validation_losses = np.array([1.4844, 1.8030, 1.8606, 1.8852, 1.8965, 1.8989])\n",
    "validation_perplexity = np.array([4.4124, 6.0680, 6.4276, 6.5876, 6.6626, 6.6784])\n",
    "episodes = np.arange(1, num_epochs + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbd96c64-40ba-4b28-997c-a0dc41e733da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/04/b5_9lzx5797dzscx6rwj46k80000gr/T/ipykernel_7019/665747474.py:4: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-gD\" (-> color='g'). The keyword argument will take precedence.\n",
      "  plt.plot(episodes, train_losses, '-gD', color = \"orange\", label='Train Loss')\n",
      "/var/folders/04/b5_9lzx5797dzscx6rwj46k80000gr/T/ipykernel_7019/665747474.py:5: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-gD\" (-> color='g'). The keyword argument will take precedence.\n",
      "  plt.plot(episodes, validation_losses, '-gD', color = \"blue\", label='Valid Loss')\n",
      "/var/folders/04/b5_9lzx5797dzscx6rwj46k80000gr/T/ipykernel_7019/665747474.py:6: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-gD\" (-> color='g'). The keyword argument will take precedence.\n",
      "  plt.plot(episodes, validation_perplexity, '-gD', color = \"purple\", label='Valid Perplexity')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Train & Valid loss & Perplexity\") \n",
    "plt.xlabel(\"Episode\") \n",
    "plt.ylabel(\"Loss / Perplexity\") \n",
    "plt.plot(episodes, train_losses, '-gD', color = \"orange\", label='Train Loss')\n",
    "plt.plot(episodes, validation_losses, '-gD', color = \"blue\", label='Valid Loss')\n",
    "plt.plot(episodes, validation_perplexity, '-gD', color = \"purple\", label='Valid Perplexity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61381284-377f-4d5c-af5e-897a4d37efa9",
   "metadata": {},
   "source": [
    "We can see here that the train loss is improving while valid loss / perplexity are getting worse. This is a sign on overfitting. One of the possible reasons of that is the fact we decreased a lot every one of the parameters in order to get the model trained with a realible amount of time, including the number of layers, and also the total number of experts and the number of the k-selected experts"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Stav Main Kernel",
   "language": "python",
   "name": "stavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
